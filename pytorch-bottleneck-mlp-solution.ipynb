{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 454.162123,
      "end_time": "2021-01-10T12:41:08.662503",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-10T12:33:34.500380",
      "version": "2.1.0"
    },
    "colab": {
      "name": "pytorch-bottleneck-mlp-solution.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.653430Z",
          "iopub.status.busy": "2021-01-10T12:33:38.652736Z",
          "iopub.status.idle": "2021-01-10T12:33:38.703037Z",
          "shell.execute_reply": "2021-01-10T12:33:38.701737Z"
        },
        "papermill": {
          "duration": 0.074553,
          "end_time": "2021-01-10T12:33:38.703178",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.628625",
          "status": "completed"
        },
        "tags": [],
        "id": "7GHDZ0YykC8A",
        "outputId": "3d8582bb-e140-46c4-c519-a39d6614640b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/jane_street\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.743996Z",
          "iopub.status.busy": "2021-01-10T12:33:38.743340Z",
          "iopub.status.idle": "2021-01-10T12:33:41.012492Z",
          "shell.execute_reply": "2021-01-10T12:33:41.011801Z"
        },
        "papermill": {
          "duration": 2.291617,
          "end_time": "2021-01-10T12:33:41.012598",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.720981",
          "status": "completed"
        },
        "tags": [],
        "id": "xDKsHz-hkC8Q",
        "outputId": "786c36f2-996b-48fc-9261-930e668bfd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.8.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.4.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (51.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.01687,
          "end_time": "2021-01-10T12:33:41.046699",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.029829",
          "status": "completed"
        },
        "tags": [],
        "id": "KoSKzu5skC8S"
      },
      "source": [
        "## 1. PurgedGroupTimeSeriesSplit\n",
        "From [here](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv), thx for sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:41.084900Z",
          "iopub.status.busy": "2021-01-10T12:33:41.084262Z",
          "iopub.status.idle": "2021-01-10T12:33:41.936379Z",
          "shell.execute_reply": "2021-01-10T12:33:41.935773Z"
        },
        "papermill": {
          "duration": 0.872721,
          "end_time": "2021-01-10T12:33:41.936488",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.063767",
          "status": "completed"
        },
        "tags": [],
        "id": "bBC8qBJVkC8T"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "\n",
        "# modified code for group gaps; source\n",
        "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\n",
        "    train into test if the model has windowed or lag features.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_group_size : int, default=Inf\n",
        "        Maximum group size for a single training set.\n",
        "    group_gap : int, default=None\n",
        "        Gap between train and test\n",
        "    max_test_group_size : int, default=Inf\n",
        "        We discard this number of groups from the end of each train split\n",
        "    \"\"\"\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_group_size=np.inf,\n",
        "                 max_test_group_size=np.inf,\n",
        "                 group_gap=None,\n",
        "                 verbose=False\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_group_size = max_train_group_size\n",
        "        self.group_gap = group_gap\n",
        "        self.max_test_group_size = max_test_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        group_gap = self.group_gap\n",
        "        max_test_group_size = self.max_test_group_size\n",
        "        max_train_group_size = self.max_train_group_size\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                \n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "\n",
        "            train_end = train_array.size\n",
        " \n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "\n",
        "            test_array  = test_array[group_gap:]\n",
        "            \n",
        "            \n",
        "            if self.verbose > 0:\n",
        "                    pass\n",
        "                    \n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016623,
          "end_time": "2021-01-10T12:33:41.970216",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.953593",
          "status": "completed"
        },
        "tags": [],
        "id": "BTwgHWZzkC8U"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:42.024868Z",
          "iopub.status.busy": "2021-01-10T12:33:42.023996Z",
          "iopub.status.idle": "2021-01-10T12:35:56.171336Z",
          "shell.execute_reply": "2021-01-10T12:35:56.170257Z"
        },
        "papermill": {
          "duration": 134.184897,
          "end_time": "2021-01-10T12:35:56.171505",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.986608",
          "status": "completed"
        },
        "tags": [],
        "id": "R_HcMfPnkC8W"
      },
      "source": [
        "# train = pd.read_csv('train.csv')\n",
        "# train = train.query('date > 85').reset_index(drop = True) \n",
        "# train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
        "# train.fillna(train.mean(),inplace=True)\n",
        "# train = train.query('weight > 0').reset_index(drop = True)\n",
        "\n",
        "# train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
        "# features = [c for c in train.columns if 'feature' in c]\n",
        "\n",
        "# resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
        "\n",
        "# X = train[features].values\n",
        "# y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
        "\n",
        "# f_mean = np.mean(train[features[1:]].values,axis=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZewWJ55x38OM"
      },
      "source": [
        "class CustomDataset:\n",
        "    def __init__(self, dataset, target):\n",
        "        self.dataset = dataset\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return {\n",
        "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
        "            'y': torch.tensor(self.target[item, :], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def load_data(PATH):\n",
        "    dt = pd.read_csv(PATH)\n",
        "    dt = pd.DataFrame(dt)\n",
        "    dt['action'] = (dt['resp'] > 0).astype('int')\n",
        "    dt.drop(columns=['resp', 'date', 'ts_id'], inplace=True)\n",
        "    \n",
        "    return dt\n",
        "\n",
        "data = load_data('train.csv')\n",
        "data.fillna(-1, inplace=True)\n",
        "target_column = 'action'\n",
        "feature_columns = data.columns[~data.columns.isin([target_column])]\n",
        "\n",
        "random_seed = 1\n",
        "learning_rate = 0.1\n",
        "num_epochs = 1\n",
        "batch_size = 2048\n",
        "num_features = len(feature_columns)\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 2\n",
        "\n",
        "train, validation = data[:int(len(data) * 0.75)], data[int(len(data) * 0.75):]\n",
        "train_data, train_target = train[feature_columns], train[[target_column]]\n",
        "validation_data, validation_target = validation[feature_columns], validation[target_column]\n",
        "train_dataset = CustomDataset(dataset=train_data.values, target=train_target.values)\n",
        "validation_dataset = CustomDataset(dataset=validation_data.values, target=validation_target.values)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.032799,
          "end_time": "2021-01-10T12:35:56.235752",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.202953",
          "status": "completed"
        },
        "tags": [],
        "id": "SL3EklSOkC8W"
      },
      "source": [
        "## 3. AutoEncoder\n",
        "THX for sharing [this great work](https://www.kaggle.com/snippsy/bottleneck-encoder-mlp-keras-tuner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.302164Z",
          "iopub.status.busy": "2021-01-10T12:35:56.296963Z",
          "iopub.status.idle": "2021-01-10T12:35:56.311109Z",
          "shell.execute_reply": "2021-01-10T12:35:56.311640Z"
        },
        "papermill": {
          "duration": 0.049526,
          "end_time": "2021-01-10T12:35:56.311774",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.262248",
          "status": "completed"
        },
        "tags": [],
        "id": "Ac5UfDgrkC8X"
      },
      "source": [
        "def softclip(tensor, min):\n",
        "    \"\"\" Clips the tensor values at the minimum value min in a softway. Taken from Handful of Trials \"\"\"\n",
        "    result_tensor = min + F.softplus(tensor - min)\n",
        "    return result_tensor\n",
        "\n",
        "class CNN_sigmaVAE(nn.Module):\n",
        "\n",
        "    def __init__(self,latent_dim=8, window_size=20, use_probabilistic_decoder=False):\n",
        "        super(CNN_sigmaVAE, self).__init__()\n",
        "        \n",
        "        self.window_size=window_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.prob_decoder = use_probabilistic_decoder\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm1d(8)\n",
        "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm1d(4)\n",
        "        \n",
        "        \n",
        "        self.fc41 = nn.Linear(4*123, self.latent_dim)\n",
        "        self.fc42 = nn.Linear(4*123, self.latent_dim)\n",
        "\n",
        "        self.defc1 = nn.Linear(self.latent_dim, 4*123)\n",
        "        \n",
        "        self.deconv1 = nn.ConvTranspose1d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn1 = nn.BatchNorm1d(16)\n",
        "        self.deconv2 = nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn2 = nn.BatchNorm1d(8)\n",
        "        self.deconv3 = nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "\n",
        "        self.log_sigma = 0\n",
        "        self.log_sigma = torch.nn.Parameter(torch.full((1,), 0.0)[0], requires_grad=True)\n",
        "        \n",
        "        \n",
        "        self.decoder_fc41 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc42 = nn.Linear(self.window_size, self.window_size)\n",
        "        \n",
        "        self.decoder_fc43 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc44 = nn.Linear(self.window_size, self.window_size)\n",
        "        \n",
        "        \n",
        "    def encoder(self, x):\n",
        "        concat_input = x #torch.cat([x, c], 1)\n",
        "        h = self.bn1(F.relu(self.conv1(concat_input)))\n",
        "        h = self.bn2(F.relu(self.conv2(h)))\n",
        "        h = self.bn3(F.relu(self.conv3(h)))\n",
        "        \n",
        "        self.saved_dim = [h.size(1), h.size(2)]\n",
        "        \n",
        "        h = h.view(h.size(0), h.size(1) * h.size(2))\n",
        "        # from IPython import embed\n",
        "        # embed()\n",
        "        return self.fc41(h), self.fc42(h)\n",
        "    \n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add(mu) # return z sample\n",
        "    \n",
        "    def decoder(self, z):\n",
        "        concat_input = z #torch.cat([z, c], 1)\n",
        "        concat_input = self.defc1(concat_input)\n",
        "        concat_input = concat_input.view(concat_input.size(0), self.saved_dim[0], self.saved_dim[1])\n",
        "\n",
        "        h = self.debn1(F.relu(self.deconv1(concat_input)))\n",
        "        h = self.debn2(F.relu(self.deconv2(h)))     \n",
        "        out = torch.sigmoid(self.deconv3(h))\n",
        "        \n",
        "        if self.prob_decoder:\n",
        "            rec_mu = self.decoder_fc43(out).tanh()\n",
        "            rec_sigma = self.decoder_fc44(out).tanh()\n",
        "            return out, rec_mu, rec_sigma\n",
        "        \n",
        "        else:\n",
        "            return out, 0, 0\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        output, rec_mu, rec_sigma = self.decoder(z)\n",
        "\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        return output, rec_mu, rec_sigma, kl_div\n",
        "\n",
        "\n",
        "    def gaussian_nll(self, mu, log_sigma, x):\n",
        "        return 0.5 * torch.pow((x - mu) / log_sigma.exp(), 2) + log_sigma + 0.5 * np.log(2 * np.pi)\n",
        "\n",
        "    \n",
        "    def reconstruction_loss(self, x_hat, x):\n",
        "\n",
        "        log_sigma = self.log_sigma\n",
        "        log_sigma = softclip(log_sigma, -6)\n",
        "        \n",
        "        rec_comps = self.gaussian_nll(x_hat, log_sigma, x)\n",
        "        rec = rec_comps.sum()\n",
        "\n",
        "        return rec_comps, rec\n",
        "\n",
        "    \n",
        "    def loss_function(self, recon_x, x, rec_mu, rec_sigma, kl):\n",
        "        \n",
        "        rec_comps, rec = self.reconstruction_loss(recon_x, x)\n",
        "        #kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        rec_mu_sigma_loss = 0\n",
        "        if self.prob_decoder:\n",
        "            rec_mu_sigma_loss = self.gaussian_nll(rec_mu, rec_sigma, x).sum()\n",
        "        \n",
        "        return rec_comps, rec, rec_mu_sigma_loss, kl\n",
        "\n",
        "\n",
        "def train_flow_model(model, num_epochs, learning_rate, dataloader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    epochs=num_epochs\n",
        "    tq = tqdm(range(epochs))\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for iteration, batch in enumerate(train_loader):\n",
        "            inputs = batch['x'].to(device)\n",
        "            labels = batch['y'].to(device)\n",
        "            labels = torch.squeeze(labels)\n",
        "\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "            # inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # from IPython import embed\n",
        "            # embed()\n",
        "            # inputs = inputs.cuda() if torch.cuda.is_available() else inputs.cpu()\n",
        "            # inputs.to(device)\n",
        "            # labels = labels.cuda() if torch.cuda.is_available() else labels.cpu()\n",
        "            # labels.to(device)\n",
        "\n",
        "            outputs, rec_mu, rec_sigma, kl = model(inputs)\n",
        "\n",
        "            _, rec, _, kl = model.loss_function(outputs, inputs, rec_mu, rec_sigma, kl)\n",
        "\n",
        "            loss = rec + kl\n",
        "\n",
        "            if(np.isnan(loss.item())):\n",
        "                print(\"Noped out at\", epoch, j, kl, rec_comps)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # tq.set_postfix(loss=loss.item())\n",
        "        print(epoch, 'total :' + str(loss.item()) + ' rec : ' + str(rec.item()) + ' kl : ' + str(kl.sum().item()) + ' sigma: ' + str(model.log_sigma.item()))\n",
        "        torch.save(model, 'bitch_street_VAE.pth')\n",
        "        torch.save(model.state_dict(), 'bitch_street_VAE_state_dict.pth')\n",
        "\n",
        "        #break\n",
        "    return model\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017009,
          "end_time": "2021-01-10T12:35:56.346494",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.329485",
          "status": "completed"
        },
        "tags": [],
        "id": "pRh2dlm4kC8Y"
      },
      "source": [
        "### 3-1  AutoEncoder Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.384797Z",
          "iopub.status.busy": "2021-01-10T12:35:56.383924Z",
          "iopub.status.idle": "2021-01-10T12:35:56.474255Z",
          "shell.execute_reply": "2021-01-10T12:35:56.473299Z"
        },
        "papermill": {
          "duration": 0.110676,
          "end_time": "2021-01-10T12:35:56.474418",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.363742",
          "status": "completed"
        },
        "tags": [],
        "id": "XDhnYd8JkC8a",
        "outputId": "8687a0a1-fc89-47b5-f611-250455d6c907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "latent_dim=8\n",
        "model = CNN_sigmaVAE(latent_dim=latent_dim)\n",
        "model.to(device)\n",
        "model.cuda() if torch.cuda.is_available() else model.cpu()\n",
        "\n",
        "model = train_flow_model(model, 1000, .001, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 total :332924.6875 rec : 326601.25 kl : 6323.4228515625 sigma: 0.24294300377368927\n",
            "1 total :290882.0 rec : 285694.0625 kl : 5187.9384765625 sigma: 0.45890340209007263\n",
            "2 total :275006.78125 rec : 270786.6875 kl : 4220.09765625 sigma: 0.6235682368278503\n",
            "3 total :269656.4375 rec : 266124.125 kl : 3532.298583984375 sigma: 0.7460553050041199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017144,
          "end_time": "2021-01-10T12:35:56.515070",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.497926",
          "status": "completed"
        },
        "tags": [],
        "id": "cFeOR0HnkC8c"
      },
      "source": [
        "## 4. MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017244,
          "end_time": "2021-01-10T12:35:56.549718",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.532474",
          "status": "completed"
        },
        "tags": [],
        "id": "MdBwqxwTkC8c"
      },
      "source": [
        "### 4-1 Define Data Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.598986Z",
          "iopub.status.busy": "2021-01-10T12:35:56.593096Z",
          "iopub.status.idle": "2021-01-10T12:35:56.603022Z",
          "shell.execute_reply": "2021-01-10T12:35:56.602204Z"
        },
        "papermill": {
          "duration": 0.035994,
          "end_time": "2021-01-10T12:35:56.603149",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.567155",
          "status": "completed"
        },
        "tags": [],
        "id": "smCpoH25kC8d"
      },
      "source": [
        "class MLP_Dataset:\n",
        "    def __init__(self, dataset, targets):\n",
        "        self.dataset = dataset\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return {\n",
        "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
        "            'y': torch.tensor(self.targets[item], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "#-------------------------------------------------------------------    \n",
        "\n",
        "class MLP_DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, targets, BATCH_SIZE, fold = None):\n",
        "        super().__init__()\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.fold = fold\n",
        "        \n",
        "    def preapre_data(self):\n",
        "        pass\n",
        "    \n",
        "    def setup(self, stage=None):\n",
        "        pass\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        \n",
        "        dataset = MLP_Dataset(dataset = self.data, targets = self.targets)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.BATCH_SIZE)\n",
        "        return train_loader\n",
        "    \n",
        "    def valid_dataloader(self):\n",
        "        dataset = MLP_Dataset(dataset = self.data, targets = self.targets)\n",
        "        valid_loader = torch.utils.data.DataLoader(dataset,batch_size=self.BATCH_SIZE)\n",
        "        return valid_loader\n",
        "\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.025897,
          "end_time": "2021-01-10T12:35:56.655596",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.629699",
          "status": "completed"
        },
        "tags": [],
        "id": "gWF5WEjjkC8e"
      },
      "source": [
        "### 4-2 Define Model and Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.723066Z",
          "iopub.status.busy": "2021-01-10T12:35:56.717546Z",
          "iopub.status.idle": "2021-01-10T12:35:56.727795Z",
          "shell.execute_reply": "2021-01-10T12:35:56.726719Z"
        },
        "papermill": {
          "duration": 0.045468,
          "end_time": "2021-01-10T12:35:56.727930",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.682462",
          "status": "completed"
        },
        "tags": [],
        "id": "iTXTAsgokC8e"
      },
      "source": [
        "# define mlp model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config, AEncoder): \n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.AEncoder = AEncoder\n",
        "        self.lr = config[\"lr\"]\n",
        "        input_shape = 260\n",
        "        \n",
        "        \n",
        "        drop_out = [config[key] for key, v in config.items() if 'dropout' in key]\n",
        "        hidden_size = [config[key] for key,v in config.items() if 'layer' in key]\n",
        "        layers = [] \n",
        "        \n",
        "        for i in range(len(hidden_size)): \n",
        "            \n",
        "            out_shape = hidden_size[i]\n",
        "                # define layers\n",
        "            layers.append(nn.Dropout(drop_out[i]))\n",
        "            layers.append(nn.Linear(input_shape, out_shape))\n",
        "            layers.append(nn.BatchNorm1d(out_shape))\n",
        "            layers.append(nn.SiLU())  # SiLU aka swish\n",
        "                # update input shape\n",
        "            input_shape = out_shape\n",
        "        \n",
        "            # define the final layer\n",
        "        layers.append(nn.Dropout(drop_out[-1]))\n",
        "        layers.append(nn.Linear(input_shape, 5))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        \n",
        "        self.model = torch.nn.Sequential(*layers)\n",
        "    \n",
        "    def encoder_decoder(self, x):\n",
        "        self.AEncoder.eval()\n",
        "        encoded = self.AEncoder(x)\n",
        "        decoded = self.AEncoder.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def forward(self, x):\n",
        "        decoded = self.encoder_decoder(x)\n",
        "        x = torch.cat((x, decoded), dim=1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.784838Z",
          "iopub.status.busy": "2021-01-10T12:35:56.772451Z",
          "iopub.status.idle": "2021-01-10T12:35:56.789443Z",
          "shell.execute_reply": "2021-01-10T12:35:56.788767Z"
        },
        "papermill": {
          "duration": 0.044061,
          "end_time": "2021-01-10T12:35:56.789549",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.745488",
          "status": "completed"
        },
        "tags": [],
        "id": "roxu7VuLkC8f"
      },
      "source": [
        "def Train(params, num_epochs=30, batch_size=4096, patience=3):\n",
        "    loss_fn = nn.BCELoss().to(device)\n",
        "    config = {**params}\n",
        "    \n",
        "    Val_Loss = 0\n",
        "    N_Samples = 0\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits[2:]):\n",
        "        print('Fold : {}'.format(fold))\n",
        "        \n",
        "    # Prepare datasets\n",
        "        # train\n",
        "        tr_x, tr_y = X[train_idx], y[train_idx]\n",
        "        train_loader = MLP_DataModule(data=tr_x, targets=tr_y, BATCH_SIZE=batch_size).train_dataloader()     \n",
        "        \n",
        "        # valid\n",
        "        val_x, val_y = X[valid_idx], y[valid_idx]\n",
        "        val_loader = MLP_DataModule(data=val_x, targets=val_y, BATCH_SIZE=batch_size).valid_dataloader()\n",
        "\n",
        "        \n",
        "        # define model\n",
        "        model = MLP(config, AEncoder).to(device) #AEncoder\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr']) \n",
        "\n",
        "        # define control variables\n",
        "        the_last_loss = 100\n",
        "        trigger_times=0\n",
        "        \n",
        "    # Training \n",
        "        for epoch in range(num_epochs):   \n",
        "            running_loss = 0.0\n",
        "            model.train()\n",
        "        \n",
        "            for batch in train_loader:\n",
        "                inputs, labels = batch['x'].to(device), batch['y'].to(device) \n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    \n",
        "            # update local train loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # update global train loss\n",
        "            epoch_loss = running_loss / len(train_loader.dataset)\n",
        "            print(' Training: Epoch({}) - Loss: {:.4f}'.format(epoch, epoch_loss))\n",
        "\n",
        "    # Validation \n",
        "            model.eval()\n",
        "            vrunning_loss = 0.0\n",
        "            num_samples = 0\n",
        "        \n",
        "            for batch in val_loader:\n",
        "                data, labels = batch['x'].to(device), batch['y'].to(device) \n",
        "                data = data.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(data)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "            \n",
        "                vrunning_loss += loss.item() * data.size(0)\n",
        "                num_samples += labels.size(0)\n",
        "\n",
        "        # update epoch loss\n",
        "            vepoch_loss = vrunning_loss/num_samples\n",
        "            print(' Validation({}) - Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
        "        \n",
        "        # Check if Early Stopping\n",
        "            if vepoch_loss > the_last_loss:\n",
        "                trigger_times += 1\n",
        "                if trigger_times >= patience:\n",
        "                    print('Meet Early stopping!')\n",
        "                    ##torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
        "                    break\n",
        "            else:\n",
        "                trigger_times = 0\n",
        "                the_last_loss = vepoch_loss\n",
        "        # Save model for the best version so far\n",
        "                torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
        "        \n",
        "    \n",
        "        # Update global loss\n",
        "        Val_Loss += vepoch_loss * num_samples\n",
        "\n",
        "        # Update global # of samples \n",
        "        N_Samples += num_samples\n",
        "        \n",
        "        # Save model if don't meet early stopping\n",
        "        torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
        "\n",
        "    return Val_Loss/N_Samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017293,
          "end_time": "2021-01-10T12:35:56.824488",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.807195",
          "status": "completed"
        },
        "tags": [],
        "id": "05alSj0_kC8g"
      },
      "source": [
        "### 4-3 Begin training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.865491Z",
          "iopub.status.busy": "2021-01-10T12:35:56.864856Z",
          "iopub.status.idle": "2021-01-10T12:36:13.980765Z",
          "shell.execute_reply": "2021-01-10T12:36:13.980164Z"
        },
        "papermill": {
          "duration": 17.138877,
          "end_time": "2021-01-10T12:36:13.980870",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.841993",
          "status": "completed"
        },
        "tags": [],
        "id": "bXSjqtfGkC8h"
      },
      "source": [
        "FOLDS=5\n",
        "gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
        "splits = list(gkf.split(y, groups=train['date'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:14.024636Z",
          "iopub.status.busy": "2021-01-10T12:36:14.023862Z",
          "iopub.status.idle": "2021-01-10T12:36:14.030819Z",
          "shell.execute_reply": "2021-01-10T12:36:14.030073Z"
        },
        "papermill": {
          "duration": 0.031771,
          "end_time": "2021-01-10T12:36:14.030917",
          "exception": false,
          "start_time": "2021-01-10T12:36:13.999146",
          "status": "completed"
        },
        "tags": [],
        "id": "YlMoC54xkC8h",
        "outputId": "74d50aac-e3c2-499e-eb45-ed3aa214b206"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:14.104747Z",
          "iopub.status.busy": "2021-01-10T12:36:14.096968Z",
          "iopub.status.idle": "2021-01-10T12:36:15.530231Z",
          "shell.execute_reply": "2021-01-10T12:36:15.530943Z"
        },
        "papermill": {
          "duration": 1.470494,
          "end_time": "2021-01-10T12:36:15.531144",
          "exception": false,
          "start_time": "2021-01-10T12:36:14.060650",
          "status": "completed"
        },
        "tags": [],
        "id": "SK6nGrIkkC8i"
      },
      "source": [
        "# Defining config\n",
        "config = {\n",
        "    \"layer_1_size\": 260,\n",
        "    \"layer_2_size\": 730,\n",
        "    \"layer_3_size\": 800,\n",
        "    \"layer_4_size\": 800,\n",
        "    \"layer_5_size\": 260,\n",
        "    \n",
        "    \"dropout_1\":0.1,\n",
        "    \"dropout_2\":0.7,\n",
        "    \"dropout_3\":0.8,\n",
        "    \"dropout_4\":0.45,\n",
        "    \"dropout_5\":0.60,\n",
        "    \"dropout_output\":0.5,\n",
        "    \"lr\": 1e-3\n",
        "}\n",
        "\n",
        "train_mode = False\n",
        "\n",
        "if train_mode:\n",
        "    model_loss = Train(config)\n",
        "    print(model_loss)\n",
        "    \n",
        "else:\n",
        "    models = []\n",
        "    for i in range(len(splits)):\n",
        "        mlp = MLP(config, AEncoder)\n",
        "        mlp.load_state_dict(torch.load(f'../input/pytorch-jsprediction-freezed-at-v20/model_{i}.pkl',map_location=torch.device('cpu')))\n",
        "        models.append(mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02789,
          "end_time": "2021-01-10T12:36:15.587362",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.559472",
          "status": "completed"
        },
        "tags": [],
        "id": "Lwwd3XyikC8j"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.650322Z",
          "iopub.status.busy": "2021-01-10T12:36:15.649623Z",
          "iopub.status.idle": "2021-01-10T12:36:15.653415Z",
          "shell.execute_reply": "2021-01-10T12:36:15.652873Z"
        },
        "papermill": {
          "duration": 0.038118,
          "end_time": "2021-01-10T12:36:15.653525",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.615407",
          "status": "completed"
        },
        "tags": [],
        "id": "7QahsNUzkC8j"
      },
      "source": [
        "# fit function\n",
        "def fit(model, x):\n",
        "    x = torch.tensor(x, dtype=torch.float)\n",
        "    model.eval()\n",
        "    pred = model(x)\n",
        "    return pred.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.697331Z",
          "iopub.status.busy": "2021-01-10T12:36:15.696692Z",
          "iopub.status.idle": "2021-01-10T12:36:15.759848Z",
          "shell.execute_reply": "2021-01-10T12:36:15.760382Z"
        },
        "papermill": {
          "duration": 0.088079,
          "end_time": "2021-01-10T12:36:15.760514",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.672435",
          "status": "completed"
        },
        "tags": [],
        "id": "Hs-y7VE_kC8j",
        "outputId": "1db83d37-fbf0-4d50-d14e-55c20be13f18"
      },
      "source": [
        "# test the predict call\n",
        "mlp.eval()\n",
        "fit(mlp, X[0:1,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6915452 , 0.66107833, 0.60748076, 0.5639223 , 0.5719603 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.801752Z",
          "iopub.status.busy": "2021-01-10T12:36:15.801156Z",
          "iopub.status.idle": "2021-01-10T12:36:15.827964Z",
          "shell.execute_reply": "2021-01-10T12:36:15.827319Z"
        },
        "papermill": {
          "duration": 0.048712,
          "end_time": "2021-01-10T12:36:15.828095",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.779383",
          "status": "completed"
        },
        "tags": [],
        "id": "IlJhLwZBkC8k"
      },
      "source": [
        "import janestreet\n",
        "env = janestreet.make_env()\n",
        "env_iter = env.iter_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.879534Z",
          "iopub.status.busy": "2021-01-10T12:36:15.876909Z",
          "iopub.status.idle": "2021-01-10T12:41:08.393388Z",
          "shell.execute_reply": "2021-01-10T12:41:08.393959Z"
        },
        "papermill": {
          "duration": 292.546472,
          "end_time": "2021-01-10T12:41:08.394171",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.847699",
          "status": "completed"
        },
        "tags": [],
        "id": "ckZO4q9qkC8k"
      },
      "source": [
        "opt_th = 0.5\n",
        "\n",
        "if not train_mode:\n",
        "    for (test_df, pred_df) in env_iter:\n",
        "        \n",
        "        if test_df['weight'].item() > 0:\n",
        "            test_df = test_df.loc[:, features].values\n",
        "            if np.isnan(test_df[:, 1:].sum()):\n",
        "                test_df[:, 1:] = np.nan_to_num(test_df[:, 1:]) + np.isnan(test_df[:, 1:]) * f_mean\n",
        "\n",
        "            pred_vector = np.mean([fit(model, test_df) for model in models],axis=0)\n",
        "            pred = np.mean(pred_vector)\n",
        "            pred_df.action = (pred_vector > opt_th).astype(int) \n",
        "            \n",
        "\n",
        "        else:\n",
        "            pred_df.action = 0\n",
        "        env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.019307,
          "end_time": "2021-01-10T12:41:08.433875",
          "exception": false,
          "start_time": "2021-01-10T12:41:08.414568",
          "status": "completed"
        },
        "tags": [],
        "id": "D5WWsEUhkC8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}