{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 454.162123,
      "end_time": "2021-01-10T12:41:08.662503",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-10T12:33:34.500380",
      "version": "2.1.0"
    },
    "colab": {
      "name": "pytorch-bottleneck-mlp-solution.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.653430Z",
          "iopub.status.busy": "2021-01-10T12:33:38.652736Z",
          "iopub.status.idle": "2021-01-10T12:33:38.703037Z",
          "shell.execute_reply": "2021-01-10T12:33:38.701737Z"
        },
        "papermill": {
          "duration": 0.074553,
          "end_time": "2021-01-10T12:33:38.703178",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.628625",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GHDZ0YykC8A",
        "outputId": "dcc4094e-a60e-4d81-e6e6-01e587cc8830"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/jane_street\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.743996Z",
          "iopub.status.busy": "2021-01-10T12:33:38.743340Z",
          "iopub.status.idle": "2021-01-10T12:33:41.012492Z",
          "shell.execute_reply": "2021-01-10T12:33:41.011801Z"
        },
        "papermill": {
          "duration": 2.291617,
          "end_time": "2021-01-10T12:33:41.012598",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.720981",
          "status": "completed"
        },
        "tags": [],
        "id": "xDKsHz-hkC8Q"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from maf import MAF, RealNVP\n",
        "from sklearn.metrics import  precision_score, recall_score, f1_score\n",
        "from IPython import embed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "LATENT_DIM = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.01687,
          "end_time": "2021-01-10T12:33:41.046699",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.029829",
          "status": "completed"
        },
        "tags": [],
        "id": "KoSKzu5skC8S"
      },
      "source": [
        "## 1. PurgedGroupTimeSeriesSplit\n",
        "From [here](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv), thx for sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:41.084900Z",
          "iopub.status.busy": "2021-01-10T12:33:41.084262Z",
          "iopub.status.idle": "2021-01-10T12:33:41.936379Z",
          "shell.execute_reply": "2021-01-10T12:33:41.935773Z"
        },
        "papermill": {
          "duration": 0.872721,
          "end_time": "2021-01-10T12:33:41.936488",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.063767",
          "status": "completed"
        },
        "tags": [],
        "id": "bBC8qBJVkC8T"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "\n",
        "# modified code for group gaps; source\n",
        "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\n",
        "    train into test if the model has windowed or lag features.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_group_size : int, default=Inf\n",
        "        Maximum group size for a single training set.\n",
        "    group_gap : int, default=None\n",
        "        Gap between train and test\n",
        "    max_test_group_size : int, default=Inf\n",
        "        We discard this number of groups from the end of each train split\n",
        "    \"\"\"\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_group_size=np.inf,\n",
        "                 max_test_group_size=np.inf,\n",
        "                 group_gap=None,\n",
        "                 verbose=False\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_group_size = max_train_group_size\n",
        "        self.group_gap = group_gap\n",
        "        self.max_test_group_size = max_test_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        group_gap = self.group_gap\n",
        "        max_test_group_size = self.max_test_group_size\n",
        "        max_train_group_size = self.max_train_group_size\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                \n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "\n",
        "            train_end = train_array.size\n",
        " \n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "\n",
        "            test_array  = test_array[group_gap:]\n",
        "            \n",
        "            \n",
        "            if self.verbose > 0:\n",
        "                    pass\n",
        "                    \n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016623,
          "end_time": "2021-01-10T12:33:41.970216",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.953593",
          "status": "completed"
        },
        "tags": [],
        "id": "BTwgHWZzkC8U"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:42.024868Z",
          "iopub.status.busy": "2021-01-10T12:33:42.023996Z",
          "iopub.status.idle": "2021-01-10T12:35:56.171336Z",
          "shell.execute_reply": "2021-01-10T12:35:56.170257Z"
        },
        "papermill": {
          "duration": 134.184897,
          "end_time": "2021-01-10T12:35:56.171505",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.986608",
          "status": "completed"
        },
        "tags": [],
        "id": "R_HcMfPnkC8W"
      },
      "source": [
        "# train = pd.read_csv('train.csv')\n",
        "# train = train.query('date > 85').reset_index(drop = True) \n",
        "# train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
        "# train.fillna(train.mean(),inplace=True)\n",
        "# train = train.query('weight > 0').reset_index(drop = True)\n",
        "\n",
        "# train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
        "# features = [c for c in train.columns if 'feature' in c]\n",
        "\n",
        "# resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
        "\n",
        "# X = train[features].values\n",
        "# y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
        "\n",
        "# f_mean = np.mean(train[features[1:]].values,axis=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ZewWJ55x38OM",
        "outputId": "f7c1a047-9a95-4d36-f44a-354e0a0b6835"
      },
      "source": [
        "class CustomDataset:\n",
        "    def __init__(self, dataset, target):\n",
        "        self.dataset = dataset\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return {\n",
        "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
        "            'y': torch.tensor(self.target[item, :], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def load_data(PATH):\n",
        "    dt = pd.read_csv(PATH)\n",
        "    dt = pd.DataFrame(dt)\n",
        "    dt['action'] = (dt['resp'] > 0).astype('int')\n",
        "    dt.drop(columns=['resp', 'date', 'ts_id'], inplace=True)\n",
        "    \n",
        "    return dt\n",
        "\n",
        "data = load_data('train.csv')\n",
        "data.fillna(-1, inplace=True)\n",
        "target_column = 'action'\n",
        "feature_columns = data.columns[~data.columns.isin([target_column])]\n",
        "\n",
        "random_seed = 1\n",
        "learning_rate = 0.1\n",
        "num_epochs = 1\n",
        "batch_size = 2048\n",
        "num_features = len(feature_columns)\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 2\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "train, validation = data[:int(len(data) * 0.75)], data[int(len(data) * 0.75):]\n",
        "\n",
        "train_data, train_target = train[feature_columns], train[[target_column]]\n",
        "validation_data, validation_target = validation[feature_columns], validation[[target_column]]\n",
        "train_dataset = CustomDataset(dataset=train_data.values, target=train_target.values)\n",
        "validation_dataset = CustomDataset(dataset=validation_data.values, target=validation_target.values)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1d63d7ff2f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.032799,
          "end_time": "2021-01-10T12:35:56.235752",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.202953",
          "status": "completed"
        },
        "tags": [],
        "id": "SL3EklSOkC8W"
      },
      "source": [
        "## 3. AutoEncoder\n",
        "THX for sharing [this great work](https://www.kaggle.com/snippsy/bottleneck-encoder-mlp-keras-tuner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.302164Z",
          "iopub.status.busy": "2021-01-10T12:35:56.296963Z",
          "iopub.status.idle": "2021-01-10T12:35:56.311109Z",
          "shell.execute_reply": "2021-01-10T12:35:56.311640Z"
        },
        "papermill": {
          "duration": 0.049526,
          "end_time": "2021-01-10T12:35:56.311774",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.262248",
          "status": "completed"
        },
        "tags": [],
        "id": "Ac5UfDgrkC8X"
      },
      "source": [
        "def softclip(tensor, min):\n",
        "    \"\"\" Clips the tensor values at the minimum value min in a softway. Taken from Handful of Trials \"\"\"\n",
        "    result_tensor = min + F.softplus(tensor - min)\n",
        "    return result_tensor\n",
        "\n",
        "class CNN_sigmaVAE(nn.Module):\n",
        "\n",
        "    def __init__(self,latent_dim=LATENT_DIM, window_size=20, use_probabilistic_decoder=False):\n",
        "        super(CNN_sigmaVAE, self).__init__()\n",
        "        \n",
        "        self.window_size=window_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.prob_decoder = use_probabilistic_decoder\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm1d(8)\n",
        "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm1d(4)\n",
        "        \n",
        "        \n",
        "        self.fc41 = nn.Linear(4*123, self.latent_dim)\n",
        "        self.fc42 = nn.Linear(4*123, self.latent_dim)\n",
        "\n",
        "        self.defc1 = nn.Linear(self.latent_dim, 4*123)\n",
        "        \n",
        "        self.deconv1 = nn.ConvTranspose1d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn1 = nn.BatchNorm1d(16)\n",
        "        self.deconv2 = nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn2 = nn.BatchNorm1d(8)\n",
        "        self.deconv3 = nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "\n",
        "        self.log_sigma = 0\n",
        "        self.log_sigma = torch.nn.Parameter(torch.full((1,), 0.0)[0], requires_grad=True)\n",
        "        \n",
        "        \n",
        "        self.decoder_fc41 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc42 = nn.Linear(self.window_size, self.window_size)\n",
        "        \n",
        "        self.decoder_fc43 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc44 = nn.Linear(self.window_size, self.window_size)\n",
        "\n",
        "        self.flow = MAF(n_blocks=1, input_size=2, cond_label_size=latent_dim, hidden_size=50, n_hidden=1)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        concat_input = x #torch.cat([x, c], 1)\n",
        "        h = self.bn1(F.relu(self.conv1(concat_input)))\n",
        "        h = self.bn2(F.relu(self.conv2(h)))\n",
        "        h = self.bn3(F.relu(self.conv3(h)))\n",
        "        \n",
        "        self.saved_dim = [h.size(1), h.size(2)]\n",
        "        \n",
        "        h = h.view(h.size(0), h.size(1) * h.size(2))\n",
        "        # from IPython import embed\n",
        "        # embed()\n",
        "        return self.fc41(h), self.fc42(h)\n",
        "    \n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add(mu) # return z sample\n",
        "    \n",
        "    def decoder(self, z):\n",
        "        concat_input = z #torch.cat([z, c], 1)\n",
        "        concat_input = self.defc1(concat_input)\n",
        "        concat_input = concat_input.view(concat_input.size(0), self.saved_dim[0], self.saved_dim[1])\n",
        "\n",
        "        h = self.debn1(F.relu(self.deconv1(concat_input)))\n",
        "        h = self.debn2(F.relu(self.deconv2(h)))     \n",
        "        out = torch.sigmoid(self.deconv3(h))\n",
        "        \n",
        "        if self.prob_decoder:\n",
        "            rec_mu = self.decoder_fc43(out).tanh()\n",
        "            rec_sigma = self.decoder_fc44(out).tanh()\n",
        "            return out, rec_mu, rec_sigma\n",
        "        \n",
        "        else:\n",
        "            return out, 0, 0\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        output, rec_mu, rec_sigma = self.decoder(z)\n",
        "\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        return output, rec_mu, rec_sigma, kl_div\n",
        "\n",
        "\n",
        "    def gaussian_nll(self, mu, log_sigma, x):\n",
        "        return 0.5 * torch.pow((x - mu) / log_sigma.exp(), 2) + log_sigma + 0.5 * np.log(2 * np.pi)\n",
        "\n",
        "    \n",
        "    def reconstruction_loss(self, x_hat, x):\n",
        "\n",
        "        log_sigma = self.log_sigma\n",
        "        log_sigma = softclip(log_sigma, -6)\n",
        "        \n",
        "        rec_comps = self.gaussian_nll(x_hat, log_sigma, x)\n",
        "        rec = rec_comps.sum()\n",
        "\n",
        "        return rec_comps, rec\n",
        "\n",
        "    \n",
        "    def loss_function(self, recon_x, x, rec_mu, rec_sigma, kl):\n",
        "        \n",
        "        rec_comps, rec = self.reconstruction_loss(recon_x, x)\n",
        "        #kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        rec_mu_sigma_loss = 0\n",
        "        if self.prob_decoder:\n",
        "            rec_mu_sigma_loss = self.gaussian_nll(rec_mu, rec_sigma, x).sum()\n",
        "        \n",
        "        return rec_comps, rec, rec_mu_sigma_loss, kl\n",
        "\n",
        "\n",
        "def train_flow_model(model, num_epochs, learning_rate, dataloader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    epochs=num_epochs\n",
        "    tq = tqdm(range(epochs))\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for iteration, batch in enumerate(dataloader):\n",
        "            inputs = batch['x'].to(device)\n",
        "            labels = batch['y'].to(device)\n",
        "            labels = torch.squeeze(labels)\n",
        "\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            mu, _ = model.encoder(inputs)\n",
        "            labels = labels.unsqueeze(1).repeat(1, 2).float()\n",
        "            zk, loss = model.flow.log_prob(x=labels, y=mu)\n",
        "\n",
        "            # VAE Training\n",
        "            # outputs, rec_mu, rec_sigma, kl = model(inputs)\n",
        "            # _, rec, _, kl = model.loss_function(outputs, inputs, rec_mu, rec_sigma, kl)\n",
        "\n",
        "            loss = -loss.mean()\n",
        "\n",
        "            if(np.isnan(loss.item())):\n",
        "                print(\"Noped out at\", epoch, j, kl, rec_comps)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # tq.set_postfix(loss=loss.item())\n",
        "        # print(epoch, 'total :' + str(loss.item()) + ' rec : ' + str(rec.item()) + ' kl : ' + str(kl.sum().item()) + ' sigma: ' + str(model.log_sigma.item()))\n",
        "        print(epoch, 'total :' + str(loss.item()))\n",
        "\n",
        "        torch.save(model, 'bitch_street_VAEflow.pth')\n",
        "        torch.save(model.state_dict(), 'bitch_street_VAEflow_state_dict.pth')\n",
        "\n",
        "        #break\n",
        "    return model\n",
        "\n",
        "def test_flow_model(model, dataloader):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    model.eval()\n",
        "    for iteration, batch in enumerate(dataloader):\n",
        "      inputs = batch['x'].to(device)\n",
        "      labels = batch['y'].to(device)\n",
        "      labels = torch.squeeze(labels)\n",
        "      # inputs = torch.from_numpy(validation_data.to_numpy()).float().to(device)\n",
        "      # labels = torch.from_numpy(validation_target.to_numpy()).float().to(device)\n",
        "\n",
        "      labels = torch.squeeze(labels)\n",
        "      inputs = inputs.unsqueeze(1)\n",
        "\n",
        "      mu, _ = model.encoder(inputs)\n",
        "      zero_test = torch.zeros([batch_size, 2]).to(device)\n",
        "      ones_test = torch.ones_like(zero_test).to(device)\n",
        "\n",
        "      _, zero_log_prob = model.flow.log_prob(x=zero_test, y=mu)\n",
        "      _, one_log_prob = model.flow.log_prob(x=ones_test, y=mu)\n",
        "\n",
        "      z = torch.zeros([batch_size, 2]).cpu()\n",
        "      z[:, 0] = zero_log_prob.cpu()\n",
        "      z[:, 1] = one_log_prob.cpu()\n",
        "\n",
        "      preds = torch.argmax(z, dim=1)\n",
        "      print(zero_log_prob[:10], one_log_prob[:10])\n",
        "      labels = labels.cpu()\n",
        "      precision = precision_score(labels[:10], preds[:10])\n",
        "      recall = recall_score(labels, preds)\n",
        "      f1 = f1_score(labels, preds)\n",
        "      print('iter: ' + str(iteration), 'precision : ' + str(precision) + ' recall : ' + str(recall) + ' f1 : ' + str(f1))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017009,
          "end_time": "2021-01-10T12:35:56.346494",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.329485",
          "status": "completed"
        },
        "tags": [],
        "id": "pRh2dlm4kC8Y"
      },
      "source": [
        "### 3-1  AutoEncoder Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.384797Z",
          "iopub.status.busy": "2021-01-10T12:35:56.383924Z",
          "iopub.status.idle": "2021-01-10T12:35:56.474255Z",
          "shell.execute_reply": "2021-01-10T12:35:56.473299Z"
        },
        "papermill": {
          "duration": 0.110676,
          "end_time": "2021-01-10T12:35:56.474418",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.363742",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XDhnYd8JkC8a",
        "outputId": "a988c646-7f49-4a47-c570-8e95b8b3057f"
      },
      "source": [
        "latent_dim=8\n",
        "model = CNN_sigmaVAE(latent_dim=latent_dim)\n",
        "model.to(device)\n",
        "model.cuda() if torch.cuda.is_available() else model.cpu()\n",
        "\n",
        "model = torch.load('bitch_street_VAEflow.pth')\n",
        "# model = train_flow_model(model, 1000, .001, train_loader)\n",
        "test_flow_model(model, validation_loader)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.2308, 3.2394, 3.2380, 3.2143, 3.1819, 3.2522, 3.2684, 3.2334, 3.2415,\n",
            "        3.2404], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3601, 2.3256, 2.3442, 2.3586, 2.3740, 2.3546, 2.3220, 2.3559, 2.3474,\n",
            "        2.3377], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 0 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2423, 3.2423, 3.2339, 3.2696, 3.2764, 3.2547, 3.2600, 3.2481, 3.2335,\n",
            "        3.2546], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3682, 2.3673, 2.4107, 2.3433, 2.3393, 2.3849, 2.3745, 2.3805, 2.3738,\n",
            "        2.3650], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 1 precision : 0.0 recall : 0.0 f1 : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([3.2682, 3.2510, 3.2577, 3.2663, 3.2425, 3.2574, 3.2514, 3.2594, 3.2623,\n",
            "        3.2472], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4127, 2.4038, 2.3971, 2.3778, 2.4163, 2.3837, 2.4054, 2.3687, 2.3843,\n",
            "        2.3779], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 2 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2248, 3.2361, 3.2005, 3.2733, 3.2537, 3.2540, 3.1919, 3.1966, 3.2158,\n",
            "        3.2406], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3330, 2.3434, 2.3644, 2.3327, 2.3003, 2.3069, 2.3824, 2.3789, 2.3616,\n",
            "        2.3052], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 3 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2204, 3.2552, 3.2515, 3.2420, 3.2195, 3.2187, 3.2373, 3.2693, 3.2483,\n",
            "        3.2309], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4146, 2.3744, 2.3950, 2.3873, 2.4228, 2.4244, 2.3995, 2.3949, 2.3929,\n",
            "        2.4136], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 4 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2119, 3.2046, 3.2439, 3.2214, 3.1993, 3.2321, 3.2252, 3.2199, 3.1958,\n",
            "        3.2546], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3589, 2.3628, 2.3063, 2.3525, 2.3576, 2.3263, 2.3420, 2.3136, 2.3862,\n",
            "        2.3190], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 5 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2524, 3.2689, 3.2744, 3.2136, 3.2477, 3.2646, 3.2912, 3.2773, 3.2405,\n",
            "        3.2607], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4171, 2.3445, 2.3812, 2.4077, 2.3726, 2.3664, 2.3333, 2.3622, 2.3734,\n",
            "        2.3864], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 6 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2548, 3.2570, 3.2389, 3.2662, 3.2382, 3.2392, 3.2416, 3.2693, 3.2307,\n",
            "        3.2488], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3452, 2.3440, 2.3582, 2.3178, 2.3632, 2.3516, 2.3488, 2.3152, 2.3586,\n",
            "        2.3629], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 7 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2225, 3.2617, 3.2587, 3.2371, 3.2595, 3.2266, 3.2400, 3.2835, 3.2368,\n",
            "        3.2184], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4035, 2.3701, 2.3877, 2.3783, 2.3918, 2.3768, 2.3873, 2.3362, 2.3776,\n",
            "        2.4043], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 8 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2775, 3.2913, 3.2582, 3.2579, 3.2524, 3.2453, 3.2313, 3.2279, 3.2493,\n",
            "        3.2529], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3600, 2.3354, 2.3618, 2.3603, 2.3889, 2.3783, 2.3936, 2.3993, 2.3924,\n",
            "        2.3628], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 9 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2220, 3.2559, 3.2465, 3.2036, 3.2692, 3.2167, 3.2341, 3.2307, 3.2490,\n",
            "        3.2147], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4045, 2.3738, 2.3862, 2.3922, 2.3673, 2.3745, 2.3705, 2.3775, 2.3641,\n",
            "        2.3669], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 10 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2688, 3.2604, 3.2555, 3.2803, 3.2632, 3.2659, 3.2387, 3.2512, 3.2572,\n",
            "        3.2790], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3785, 2.3732, 2.3694, 2.3460, 2.3735, 2.3684, 2.3910, 2.3724, 2.3885,\n",
            "        2.3565], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 11 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2715, 3.2851, 3.2342, 3.2426, 3.2611, 3.2613, 3.2451, 3.2398, 3.2340,\n",
            "        3.2459], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3583, 2.3137, 2.3802, 2.3990, 2.3311, 2.3219, 2.3482, 2.3865, 2.3935,\n",
            "        2.3899], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 12 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2364, 3.2566, 3.2295, 3.2398, 3.2352, 3.2574, 3.2513, 3.2374, 3.2789,\n",
            "        3.2505], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4057, 2.4037, 2.4022, 2.4098, 2.3827, 2.4145, 2.3739, 2.4035, 2.3618,\n",
            "        2.3927], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 13 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2376, 3.1915, 3.2692, 3.2521, 3.2363, 3.2199, 3.2550, 3.2330, 3.2675,\n",
            "        3.2243], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3650, 2.3972, 2.3317, 2.3864, 2.3613, 2.3845, 2.3214, 2.3490, 2.3419,\n",
            "        2.3588], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 14 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2241, 3.2395, 3.2607, 3.2610, 3.2793, 3.2702, 3.2564, 3.2285, 3.2524,\n",
            "        3.2627], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3974, 2.4001, 2.3726, 2.3579, 2.3478, 2.3488, 2.3763, 2.4039, 2.3922,\n",
            "        2.3483], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 15 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1997, 3.1959, 3.2057, 3.2112, 3.2580, 3.2724, 3.2551, 3.2436, 3.2248,\n",
            "        3.2117], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3647, 2.3597, 2.3528, 2.3490, 2.3206, 2.2990, 2.3285, 2.3435, 2.3520,\n",
            "        2.3889], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 16 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2743, 3.2631, 3.2367, 3.2361, 3.2335, 3.2658, 3.2733, 3.1948, 3.2714,\n",
            "        3.2236], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3794, 2.3916, 2.4025, 2.3894, 2.3909, 2.3965, 2.3435, 2.4132, 2.3455,\n",
            "        2.3913], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 17 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2264, 3.2065, 3.2404, 3.2289, 3.2628, 3.2191, 3.2496, 3.2349, 3.2429,\n",
            "        3.2720], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3670, 2.3887, 2.3383, 2.3689, 2.3487, 2.3669, 2.3412, 2.3766, 2.3372,\n",
            "        2.3781], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 18 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2449, 3.2615, 3.2805, 3.2332, 3.2621, 3.2793, 3.2605, 3.2258, 3.2319,\n",
            "        3.2109], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3810, 2.3781, 2.3485, 2.3747, 2.3493, 2.3908, 2.3923, 2.3993, 2.4035,\n",
            "        2.4157], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 19 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2361, 3.2565, 3.2548, 3.1969, 3.2522, 3.2481, 3.2694, 3.2478, 3.2675,\n",
            "        3.2544], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3824, 2.3500, 2.3396, 2.3830, 2.3697, 2.3371, 2.3498, 2.3660, 2.3203,\n",
            "        2.3653], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 20 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2755, 3.2331, 3.2749, 3.2502, 3.2870, 3.2455, 3.2495, 3.2311, 3.2635,\n",
            "        3.2418], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3755, 2.3825, 2.3485, 2.3988, 2.3056, 2.3900, 2.3886, 2.4152, 2.3626,\n",
            "        2.3830], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 21 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2598, 3.2594, 3.2648, 3.2329, 3.2559, 3.2625, 3.2563, 3.2538, 3.2674,\n",
            "        3.2382], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3664, 2.3331, 2.3269, 2.3356, 2.3230, 2.3159, 2.3428, 2.3684, 2.3539,\n",
            "        2.3750], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 22 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2656, 3.2576, 3.2488, 3.2524, 3.2321, 3.2341, 3.2331, 3.2527, 3.2880,\n",
            "        3.2195], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3709, 2.3783, 2.3669, 2.3675, 2.4092, 2.4123, 2.4007, 2.3666, 2.3469,\n",
            "        2.3937], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 23 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2345, 3.2633, 3.2229, 3.2638, 3.2531, 3.2508, 3.2121, 3.2679, 3.2561,\n",
            "        3.2254], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3937, 2.3818, 2.4002, 2.3708, 2.3554, 2.3750, 2.3979, 2.3732, 2.3858,\n",
            "        2.4029], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 24 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2206, 3.2389, 3.2343, 3.2299, 3.2114, 3.2319, 3.2383, 3.2628, 3.2237,\n",
            "        3.2118], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3792, 2.3664, 2.3527, 2.3461, 2.3688, 2.3591, 2.3677, 2.3408, 2.3588,\n",
            "        2.3730], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 25 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2602, 3.2500, 3.2483, 3.2088, 3.2626, 3.2638, 3.2494, 3.2198, 3.2204,\n",
            "        3.2782], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3595, 2.3748, 2.3605, 2.3931, 2.3478, 2.3713, 2.3814, 2.4072, 2.3975,\n",
            "        2.3095], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 26 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2589, 3.2710, 3.2614, 3.2584, 3.1905, 3.2839, 3.2460, 3.2251, 3.2091,\n",
            "        3.2264], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3345, 2.3552, 2.3725, 2.3513, 2.4156, 2.3417, 2.3948, 2.4084, 2.3942,\n",
            "        2.3803], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 27 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2483, 3.2416, 3.2285, 3.2289, 3.2383, 3.2288, 3.1993, 3.2044, 3.1954,\n",
            "        3.2644], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3292, 2.3271, 2.3503, 2.3413, 2.3438, 2.3397, 2.3538, 2.3589, 2.3804,\n",
            "        2.3335], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 28 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2240, 3.2241, 3.2181, 3.2609, 3.2563, 3.2481, 3.2448, 3.2228, 3.2247,\n",
            "        3.2436], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4129, 2.4016, 2.4089, 2.3893, 2.3619, 2.3630, 2.3775, 2.3914, 2.3882,\n",
            "        2.4064], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 29 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2689, 3.2415, 3.2336, 3.2636, 3.2419, 3.2156, 3.2103, 3.2024, 3.2294,\n",
            "        3.2428], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3549, 2.3917, 2.3902, 2.3802, 2.3919, 2.4037, 2.4429, 2.4185, 2.4071,\n",
            "        2.4096], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 30 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2520, 3.2273, 3.2306, 3.2368, 3.2473, 3.2256, 3.2688, 3.2276, 3.2367,\n",
            "        3.2627], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3252, 2.3668, 2.3454, 2.3587, 2.3187, 2.3554, 2.3085, 2.3742, 2.3512,\n",
            "        2.3377], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 31 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2389, 3.2156, 3.2592, 3.2651, 3.2428, 3.2384, 3.2783, 3.2230, 3.2573,\n",
            "        3.2258], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3711, 2.4102, 2.3666, 2.3731, 2.4058, 2.3960, 2.3607, 2.4108, 2.3914,\n",
            "        2.4052], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 32 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2143, 3.2385, 3.2507, 3.2339, 3.2364, 3.2572, 3.2486, 3.2819, 3.2470,\n",
            "        3.2836], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4172, 2.3796, 2.3716, 2.3639, 2.3453, 2.3362, 2.3483, 2.3196, 2.3654,\n",
            "        2.3128], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 33 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2247, 3.2482, 3.2318, 3.2731, 3.2270, 3.2779, 3.2186, 3.2754, 3.2614,\n",
            "        3.2358], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3990, 2.3635, 2.3631, 2.3231, 2.3964, 2.3568, 2.4098, 2.3206, 2.3805,\n",
            "        2.3938], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 34 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2360, 3.2632, 3.2744, 3.2411, 3.2901, 3.2846, 3.2504, 3.2406, 3.2136,\n",
            "        3.2558], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3879, 2.3798, 2.3592, 2.3980, 2.3726, 2.3687, 2.3648, 2.4038, 2.4064,\n",
            "        2.3551], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 35 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2496, 3.2776, 3.2688, 3.2566, 3.2586, 3.2070, 3.1959, 3.2139, 3.2577,\n",
            "        3.2294], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3072, 2.3218, 2.3510, 2.3567, 2.3412, 2.3915, 2.4165, 2.3833, 2.3112,\n",
            "        2.3779], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 36 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2413, 3.2349, 3.2086, 3.2763, 3.1904, 3.2176, 3.2346, 3.2297, 3.2746,\n",
            "        3.2179], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4278, 2.4091, 2.4065, 2.3782, 2.4174, 2.3911, 2.3955, 2.3787, 2.3546,\n",
            "        2.4166], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 37 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2283, 3.2607, 3.2608, 3.2020, 3.2500, 3.2340, 3.1983, 3.2146, 3.2384,\n",
            "        3.2493], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3798, 2.3394, 2.3149, 2.3729, 2.3560, 2.3830, 2.4074, 2.4033, 2.3385,\n",
            "        2.3629], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 38 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2326, 3.2329, 3.2260, 3.2465, 3.2644, 3.2748, 3.2212, 3.2503, 3.2322,\n",
            "        3.2658], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4206, 2.4019, 2.4124, 2.3715, 2.3802, 2.3607, 2.4120, 2.4109, 2.3757,\n",
            "        2.3590], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 39 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2531, 3.2508, 3.2413, 3.2432, 3.2128, 3.2532, 3.2081, 3.2138, 3.2294,\n",
            "        3.2276], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3387, 2.3197, 2.3503, 2.3270, 2.3755, 2.3295, 2.3576, 2.3671, 2.3777,\n",
            "        2.3553], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 40 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2747, 3.2640, 3.2653, 3.2632, 3.2627, 3.2467, 3.2413, 3.2358, 3.2538,\n",
            "        3.2497], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3244, 2.3531, 2.3657, 2.3918, 2.3519, 2.3877, 2.4085, 2.3941, 2.3848,\n",
            "        2.3473], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 41 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1963, 3.2079, 3.2629, 3.2493, 3.2265, 3.2109, 3.2536, 3.2009, 3.1883,\n",
            "        3.2238], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3487, 2.3490, 2.3196, 2.3258, 2.3450, 2.3598, 2.3260, 2.3847, 2.3549,\n",
            "        2.3207], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 42 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2635, 3.2417, 3.2510, 3.2588, 3.2291, 3.2549, 3.2722, 3.2495, 3.2720,\n",
            "        3.2603], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3544, 2.4057, 2.3451, 2.3914, 2.4073, 2.4018, 2.3390, 2.3655, 2.3819,\n",
            "        2.3749], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 43 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2682, 3.2339, 3.2155, 3.2097, 3.2092, 3.2230, 3.2078, 3.2315, 3.2358,\n",
            "        3.2550], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3369, 2.3369, 2.3601, 2.3829, 2.3706, 2.3820, 2.3649, 2.3428, 2.3381,\n",
            "        2.3251], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 44 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2531, 3.2251, 3.2398, 3.2652, 3.2748, 3.2548, 3.2363, 3.2954, 3.2852,\n",
            "        3.2597], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3792, 2.4005, 2.3772, 2.3678, 2.3985, 2.3834, 2.3858, 2.3406, 2.3610,\n",
            "        2.4113], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 45 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2303, 3.2449, 3.2252, 3.2531, 3.2411, 3.2299, 3.2346, 3.2531, 3.2400,\n",
            "        3.2437], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3669, 2.3221, 2.3553, 2.3458, 2.3366, 2.3774, 2.3402, 2.3578, 2.3557,\n",
            "        2.3437], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 46 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2741, 3.2655, 3.1999, 3.2243, 3.2312, 3.2752, 3.2599, 3.2340, 3.1971,\n",
            "        3.2545], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3980, 2.3455, 2.4339, 2.3895, 2.3922, 2.3581, 2.3744, 2.3937, 2.4252,\n",
            "        2.3633], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 47 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2325, 3.2539, 3.2409, 3.2500, 3.2514, 3.2437, 3.2388, 3.2670, 3.2313,\n",
            "        3.2556], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3464, 2.3737, 2.3405, 2.3278, 2.3370, 2.3177, 2.3333, 2.3142, 2.3585,\n",
            "        2.3066], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 48 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2495, 3.2494, 3.2590, 3.2776, 3.2384, 3.2510, 3.2535, 3.2310, 3.2383,\n",
            "        3.2076], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3768, 2.3915, 2.3727, 2.3468, 2.4033, 2.3805, 2.4002, 2.3767, 2.3738,\n",
            "        2.3971], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 49 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2303, 3.2817, 3.2040, 3.2021, 3.2191, 3.2416, 3.2717, 3.2421, 3.2728,\n",
            "        3.2532], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3960, 2.3563, 2.4207, 2.3885, 2.3856, 2.3765, 2.3699, 2.3855, 2.3713,\n",
            "        2.3778], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 50 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2504, 3.2288, 3.2657, 3.2303, 3.2421, 3.2465, 3.2525, 3.2580, 3.2744,\n",
            "        3.2010], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3719, 2.3910, 2.3746, 2.3792, 2.3708, 2.3671, 2.3568, 2.3630, 2.3519,\n",
            "        2.4127], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 51 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2369, 3.2153, 3.2145, 3.2219, 3.2810, 3.2560, 3.2446, 3.2654, 3.2241,\n",
            "        3.2714], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3335, 2.4106, 2.4127, 2.4024, 2.3725, 2.3734, 2.3938, 2.3411, 2.3811,\n",
            "        2.3345], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 52 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2443, 3.1779, 3.2729, 3.2234, 3.2390, 3.2450, 3.2787, 3.2315, 3.2334,\n",
            "        3.2245], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3711, 2.3976, 2.3416, 2.3541, 2.3434, 2.3145, 2.3131, 2.3717, 2.3597,\n",
            "        2.3862], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 53 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2391, 3.2544, 3.2673, 3.2647, 3.2193, 3.2096, 3.2485, 3.2184, 3.2525,\n",
            "        3.2542], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4075, 2.3756, 2.3578, 2.3621, 2.3975, 2.3974, 2.3913, 2.3815, 2.4009,\n",
            "        2.3877], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 54 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2276, 3.2611, 3.2334, 3.2338, 3.2529, 3.2460, 3.2539, 3.2391, 3.2430,\n",
            "        3.2317], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3541, 2.3346, 2.3754, 2.3924, 2.3803, 2.3707, 2.3655, 2.3751, 2.3633,\n",
            "        2.3683], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 55 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2497, 3.2354, 3.2604, 3.2219, 3.2576, 3.2583, 3.2325, 3.2596, 3.2609,\n",
            "        3.2331], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3597, 2.4007, 2.4014, 2.4174, 2.3564, 2.3771, 2.4023, 2.3639, 2.3648,\n",
            "        2.4007], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 56 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2394, 3.2434, 3.2317, 3.2580, 3.2175, 3.2392, 3.2613, 3.2420, 3.1990,\n",
            "        3.2471], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3473, 2.3237, 2.3354, 2.3206, 2.3300, 2.3375, 2.3195, 2.3389, 2.3992,\n",
            "        2.3545], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 57 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2182, 3.2321, 3.2152, 3.2721, 3.2526, 3.2347, 3.2492, 3.2567, 3.2630,\n",
            "        3.2403], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4137, 2.4225, 2.4192, 2.3505, 2.3729, 2.3820, 2.3625, 2.3631, 2.3603,\n",
            "        2.4054], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 58 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1799, 3.2449, 3.2282, 3.2765, 3.2303, 3.2425, 3.2397, 3.2143, 3.1998,\n",
            "        3.2125], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4093, 2.3733, 2.3902, 2.3357, 2.3700, 2.4010, 2.3860, 2.4237, 2.4022,\n",
            "        2.3941], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 59 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2008, 3.2480, 3.2799, 3.2621, 3.2272, 3.1956, 3.2272, 3.2526, 3.2441,\n",
            "        3.2537], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4101, 2.3433, 2.3356, 2.3366, 2.3555, 2.3923, 2.4018, 2.3496, 2.3729,\n",
            "        2.3206], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 60 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2462, 3.2621, 3.2685, 3.2223, 3.2329, 3.2478, 3.2190, 3.2290, 3.2365,\n",
            "        3.2240], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3773, 2.3704, 2.3829, 2.3967, 2.4065, 2.3815, 2.4174, 2.4043, 2.4013,\n",
            "        2.3864], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 61 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2506, 3.2070, 3.2620, 3.2202, 3.2302, 3.2240, 3.2176, 3.2139, 3.2372,\n",
            "        3.2178], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3552, 2.3404, 2.3297, 2.3526, 2.3347, 2.3703, 2.3597, 2.3658, 2.3600,\n",
            "        2.3577], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 62 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2254, 3.2602, 3.2683, 3.2414, 3.2306, 3.2673, 3.2461, 3.2572, 3.2521,\n",
            "        3.2535], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4242, 2.3687, 2.3913, 2.4061, 2.4106, 2.3465, 2.4053, 2.3654, 2.3773,\n",
            "        2.4015], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 63 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2302, 3.2341, 3.2253, 3.2426, 3.2418, 3.2193, 3.2320, 3.2298, 3.2460,\n",
            "        3.2732], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3865, 2.3897, 2.3587, 2.3473, 2.3563, 2.3752, 2.3691, 2.3644, 2.3546,\n",
            "        2.3178], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 64 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2466, 3.2674, 3.2584, 3.2486, 3.2426, 3.2420, 3.2659, 3.2346, 3.2040,\n",
            "        3.2280], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3966, 2.3519, 2.3741, 2.3908, 2.3690, 2.3880, 2.3842, 2.3860, 2.3902,\n",
            "        2.3814], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 65 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2273, 3.2486, 3.2391, 3.2513, 3.2605, 3.2460, 3.2552, 3.2308, 3.2167,\n",
            "        3.2642], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3648, 2.3403, 2.3265, 2.3526, 2.3517, 2.3919, 2.3624, 2.3674, 2.3970,\n",
            "        2.3349], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 66 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2287, 3.2680, 3.2728, 3.2677, 3.2193, 3.2641, 3.2722, 3.2213, 3.2421,\n",
            "        3.2428], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3920, 2.3578, 2.3640, 2.3714, 2.4027, 2.3414, 2.3593, 2.3912, 2.4141,\n",
            "        2.3858], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 67 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2178, 3.2203, 3.2523, 3.2361, 3.2675, 3.2143, 3.2550, 3.2543, 3.2633,\n",
            "        3.1965], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3410, 2.3525, 2.3291, 2.3359, 2.3042, 2.3355, 2.3271, 2.3038, 2.3344,\n",
            "        2.3664], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 68 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2328, 3.2103, 3.2638, 3.2810, 3.2416, 3.2486, 3.2389, 3.2789, 3.2275,\n",
            "        3.2670], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4026, 2.4003, 2.3618, 2.3372, 2.3825, 2.3786, 2.4009, 2.3640, 2.3921,\n",
            "        2.3719], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 69 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2319, 3.2364, 3.2253, 3.2374, 3.2227, 3.2489, 3.2438, 3.2245, 3.2180,\n",
            "        3.2871], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3548, 2.3566, 2.3528, 2.3410, 2.3564, 2.3404, 2.3375, 2.3451, 2.3617,\n",
            "        2.2993], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 70 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2444, 3.2582, 3.2791, 3.2256, 3.2414, 3.2560, 3.2543, 3.2162, 3.2792,\n",
            "        3.2404], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3706, 2.3346, 2.3372, 2.4007, 2.3830, 2.4008, 2.3771, 2.3920, 2.3475,\n",
            "        2.3875], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 71 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2404, 3.2258, 3.2252, 3.2786, 3.2321, 3.2212, 3.2310, 3.2170, 3.2257,\n",
            "        3.2093], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3911, 2.3737, 2.3790, 2.3529, 2.3564, 2.3511, 2.4088, 2.3699, 2.3771,\n",
            "        2.3678], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 72 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2230, 3.2573, 3.2349, 3.2200, 3.2216, 3.2222, 3.2290, 3.2404, 3.2448,\n",
            "        3.2402], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3656, 2.3202, 2.3496, 2.3704, 2.3764, 2.3873, 2.3569, 2.3707, 2.3375,\n",
            "        2.3315], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 73 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2909, 3.2696, 3.2820, 3.2231, 3.2744, 3.2732, 3.2706, 3.2596, 3.2042,\n",
            "        3.2139], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3886, 2.3320, 2.3538, 2.4035, 2.3572, 2.3634, 2.3701, 2.3968, 2.4137,\n",
            "        2.4096], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 74 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2263, 3.2116, 3.2454, 3.2133, 3.2635, 3.2566, 3.2541, 3.2574, 3.2704,\n",
            "        3.2468], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3469, 2.3627, 2.3303, 2.3706, 2.3066, 2.3253, 2.3252, 2.3267, 2.3012,\n",
            "        2.3355], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 75 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2687, 3.2235, 3.2023, 3.2563, 3.2424, 3.2667, 3.2785, 3.2445, 3.2787,\n",
            "        3.2734], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3620, 2.3797, 2.4057, 2.3700, 2.3975, 2.3497, 2.3467, 2.3725, 2.3562,\n",
            "        2.3542], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 76 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2550, 3.2467, 3.2428, 3.2682, 3.2611, 3.2461, 3.2273, 3.2828, 3.2907,\n",
            "        3.2576], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3696, 2.3660, 2.4037, 2.3476, 2.3876, 2.3832, 2.4049, 2.3590, 2.3598,\n",
            "        2.4202], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 77 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2100, 3.2538, 3.2743, 3.2686, 3.2883, 3.2574, 3.2788, 3.2315, 3.2510,\n",
            "        3.2552], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3729, 2.3667, 2.3563, 2.3566, 2.3243, 2.3345, 2.3317, 2.3849, 2.3660,\n",
            "        2.3794], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 78 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2355, 3.2820, 3.2612, 3.2426, 3.2621, 3.2798, 3.2757, 3.2684, 3.2858,\n",
            "        3.2773], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4027, 2.3748, 2.3691, 2.3949, 2.3683, 2.3564, 2.3417, 2.3890, 2.3401,\n",
            "        2.3449], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 79 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2286, 3.2862, 3.2575, 3.2459, 3.2479, 3.2254, 3.2531, 3.2169, 3.2257,\n",
            "        3.2288], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3783, 2.3190, 2.3492, 2.3863, 2.3672, 2.3823, 2.3496, 2.3784, 2.3791,\n",
            "        2.4010], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 80 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2605, 3.2613, 3.2584, 3.2236, 3.2586, 3.2364, 3.2169, 3.2604, 3.2686,\n",
            "        3.2693], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4111, 2.3830, 2.3887, 2.4053, 2.3887, 2.3771, 2.3971, 2.3500, 2.3714,\n",
            "        2.3645], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 81 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2334, 3.2747, 3.2540, 3.2504, 3.2203, 3.2635, 3.2677, 3.2234, 3.2565,\n",
            "        3.2177], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3828, 2.3550, 2.3782, 2.3647, 2.4023, 2.3497, 2.3571, 2.3907, 2.3483,\n",
            "        2.3947], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 82 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2530, 3.2736, 3.2705, 3.2245, 3.2620, 3.2671, 3.2480, 3.2438, 3.2636,\n",
            "        3.2305], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3791, 2.3565, 2.3499, 2.4201, 2.3478, 2.3687, 2.4267, 2.4111, 2.3942,\n",
            "        2.3901], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 83 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2665, 3.2540, 3.2512, 3.2449, 3.2182, 3.2535, 3.2264, 3.2362, 3.2503,\n",
            "        3.2464], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3349, 2.3732, 2.3517, 2.3677, 2.3797, 2.3880, 2.3785, 2.3778, 2.3620,\n",
            "        2.3663], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 84 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2292, 3.2611, 3.2138, 3.2576, 3.2658, 3.2725, 3.2829, 3.2494, 3.2162,\n",
            "        3.2436], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4089, 2.3931, 2.4108, 2.3948, 2.3685, 2.3659, 2.3450, 2.3889, 2.4013,\n",
            "        2.3914], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 85 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2381, 3.2430, 3.2828, 3.2660, 3.2053, 3.2559, 3.2719, 3.2239, 3.2491,\n",
            "        3.2621], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3624, 2.3626, 2.3383, 2.3543, 2.3879, 2.3447, 2.3362, 2.3524, 2.3628,\n",
            "        2.3490], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 86 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2685, 3.2383, 3.2420, 3.2632, 3.2632, 3.2367, 3.2354, 3.2623, 3.2247,\n",
            "        3.2414], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3626, 2.3763, 2.3887, 2.3638, 2.3717, 2.3978, 2.3707, 2.3671, 2.3986,\n",
            "        2.3845], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 87 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2402, 3.2450, 3.2458, 3.2647, 3.2608, 3.2483, 3.2763, 3.2280, 3.2760,\n",
            "        3.2256], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3241, 2.3617, 2.3694, 2.3267, 2.3511, 2.3524, 2.3899, 2.3547, 2.3189,\n",
            "        2.3887], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 88 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2418, 3.2698, 3.2431, 3.2710, 3.2686, 3.2719, 3.2501, 3.2432, 3.2279,\n",
            "        3.2267], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3986, 2.3738, 2.3962, 2.3681, 2.3619, 2.3544, 2.3822, 2.4026, 2.4012,\n",
            "        2.4051], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 89 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2290, 3.2087, 3.2632, 3.2153, 3.1960, 3.1997, 3.2409, 3.2428, 3.2448,\n",
            "        3.2251], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3881, 2.3959, 2.3289, 2.3779, 2.3933, 2.3781, 2.3714, 2.3638, 2.3657,\n",
            "        2.3816], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 90 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2901, 3.2464, 3.2601, 3.2104, 3.2759, 3.2569, 3.2565, 3.2624, 3.2141,\n",
            "        3.2533], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3269, 2.3814, 2.3758, 2.4225, 2.3618, 2.3921, 2.3665, 2.3605, 2.4157,\n",
            "        2.3788], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 91 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2534, 3.2661, 3.2392, 3.2578, 3.1886, 3.2255, 3.2612, 3.2398, 3.2746,\n",
            "        3.2569], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3360, 2.3304, 2.3586, 2.3515, 2.3753, 2.3669, 2.3209, 2.3416, 2.3105,\n",
            "        2.3315], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 92 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2606, 3.2710, 3.2758, 3.2652, 3.2288, 3.2464, 3.2460, 3.2480, 3.2594,\n",
            "        3.2724], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3751, 2.3989, 2.3528, 2.3767, 2.4189, 2.3665, 2.3701, 2.3817, 2.3461,\n",
            "        2.3609], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 93 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2657, 3.2678, 3.2366, 3.2640, 3.2699, 3.2578, 3.2614, 3.2341, 3.2224,\n",
            "        3.2785], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3753, 2.3546, 2.4362, 2.3737, 2.3846, 2.3765, 2.4113, 2.4190, 2.4274,\n",
            "        2.3554], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 94 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2317, 3.2458, 3.2569, 3.2582, 3.2896, 3.2253, 3.2519, 3.2426, 3.2491,\n",
            "        3.2554], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3823, 2.3761, 2.3485, 2.3547, 2.3044, 2.4011, 2.3644, 2.3949, 2.3397,\n",
            "        2.3387], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 95 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2512, 3.2782, 3.2658, 3.2421, 3.2427, 3.2588, 3.2386, 3.2977, 3.2548,\n",
            "        3.2690], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3801, 2.3568, 2.3658, 2.3986, 2.3778, 2.3793, 2.3916, 2.3382, 2.3877,\n",
            "        2.3867], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 96 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2464, 3.2231, 3.2238, 3.2205, 3.2647, 3.2299, 3.2669, 3.2469, 3.2382,\n",
            "        3.2394], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3390, 2.3295, 2.3755, 2.3480, 2.3407, 2.3578, 2.3273, 2.3684, 2.3234,\n",
            "        2.3413], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 97 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2856, 3.2466, 3.2458, 3.2344, 3.2331, 3.2282, 3.2475, 3.2185, 3.2738,\n",
            "        3.2085], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3238, 2.3996, 2.4072, 2.4121, 2.4068, 2.4050, 2.3441, 2.4031, 2.3647,\n",
            "        2.4102], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 98 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2028, 3.2042, 3.2392, 3.2660, 3.2451, 3.2064, 3.2364, 3.2106, 3.2676,\n",
            "        3.2455], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3515, 2.3455, 2.3353, 2.3358, 2.3461, 2.3672, 2.3369, 2.3896, 2.3349,\n",
            "        2.3462], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 99 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2764, 3.2523, 3.2532, 3.2651, 3.2413, 3.2342, 3.2621, 3.2602, 3.2804,\n",
            "        3.2250], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3584, 2.3896, 2.3829, 2.3808, 2.3823, 2.3776, 2.3763, 2.3777, 2.3422,\n",
            "        2.4002], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 100 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2504, 3.2724, 3.2335, 3.2458, 3.2184, 3.2313, 3.2185, 3.2505, 3.2613,\n",
            "        3.2583], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3263, 2.2984, 2.3350, 2.3450, 2.3527, 2.3521, 2.3682, 2.3356, 2.3276,\n",
            "        2.3158], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 101 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2401, 3.2119, 3.2251, 3.1867, 3.2532, 3.2297, 3.2250, 3.2261, 3.2153,\n",
            "        3.2440], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3777, 2.3815, 2.3860, 2.3954, 2.3622, 2.3574, 2.3859, 2.3867, 2.3943,\n",
            "        2.3754], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 102 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2907, 3.2701, 3.2659, 3.2548, 3.2600, 3.2683, 3.2216, 3.2912, 3.2866,\n",
            "        3.2551], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3693, 2.3575, 2.3783, 2.3631, 2.3592, 2.3953, 2.4042, 2.3394, 2.3544,\n",
            "        2.3812], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 103 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2414, 3.2192, 3.2308, 3.1933, 3.2293, 3.2158, 3.2842, 3.2480, 3.2471,\n",
            "        3.2835], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3921, 2.4115, 2.4082, 2.4246, 2.3965, 2.4096, 2.3548, 2.4038, 2.3902,\n",
            "        2.3367], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 104 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2150, 3.2187, 3.2457, 3.2619, 3.2370, 3.2553, 3.2635, 3.2741, 3.1953,\n",
            "        3.2138], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3629, 2.3774, 2.3592, 2.3503, 2.3649, 2.3207, 2.3249, 2.3624, 2.3701,\n",
            "        2.3735], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 105 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2573, 3.2382, 3.2440, 3.2781, 3.2672, 3.2354, 3.2547, 3.2460, 3.2386,\n",
            "        3.2660], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3627, 2.4103, 2.3871, 2.3687, 2.3876, 2.3761, 2.3429, 2.3664, 2.3873,\n",
            "        2.3564], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 106 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2824, 3.2397, 3.2621, 3.2685, 3.2255, 3.2260, 3.2596, 3.2811, 3.2370,\n",
            "        3.2350], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3123, 2.3464, 2.3290, 2.3137, 2.3701, 2.3505, 2.3247, 2.3126, 2.3371,\n",
            "        2.3315], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 107 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2786, 3.2684, 3.2198, 3.2747, 3.2740, 3.2388, 3.2274, 3.1978, 3.2189,\n",
            "        3.2598], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3175, 2.3516, 2.3839, 2.3245, 2.3947, 2.4126, 2.3760, 2.4012, 2.3885,\n",
            "        2.3440], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 108 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2599, 3.2629, 3.2428, 3.2445, 3.2768, 3.2472, 3.2696, 3.2568, 3.2675,\n",
            "        3.2134], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3896, 2.3676, 2.4010, 2.3900, 2.3828, 2.3880, 2.3604, 2.3735, 2.3873,\n",
            "        2.3944], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 109 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2363, 3.2449, 3.2586, 3.2482, 3.2353, 3.2422, 3.2183, 3.2618, 3.2526,\n",
            "        3.2078], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3827, 2.3579, 2.2998, 2.3600, 2.3630, 2.3268, 2.3757, 2.3612, 2.3640,\n",
            "        2.3662], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 110 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2704, 3.2589, 3.2800, 3.2618, 3.2105, 3.2271, 3.2745, 3.2477, 3.2235,\n",
            "        3.2363], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3556, 2.3554, 2.3131, 2.3519, 2.4199, 2.3740, 2.3768, 2.3783, 2.4053,\n",
            "        2.3834], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 111 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2532, 3.2211, 3.2312, 3.2422, 3.2059, 3.2263, 3.2000, 3.2216, 3.2747,\n",
            "        3.2435], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3812, 2.4175, 2.3963, 2.4041, 2.4149, 2.4033, 2.4090, 2.4015, 2.3696,\n",
            "        2.3985], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 112 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2143, 3.2579, 3.2784, 3.2513, 3.2554, 3.2611, 3.2227, 3.2623, 3.2008,\n",
            "        3.2037], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3681, 2.3570, 2.3094, 2.3507, 2.3574, 2.3490, 2.4024, 2.3721, 2.3952,\n",
            "        2.3650], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 113 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2672, 3.2579, 3.2538, 3.2727, 3.2194, 3.2626, 3.2717, 3.2838, 3.2247,\n",
            "        3.2169], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3839, 2.4166, 2.3765, 2.3627, 2.4181, 2.3769, 2.3481, 2.3412, 2.4276,\n",
            "        2.4156], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 114 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2206, 3.2577, 3.2519, 3.2088, 3.2469, 3.2606, 3.2397, 3.2447, 3.2810,\n",
            "        3.1913], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3951, 2.3229, 2.3401, 2.3577, 2.3834, 2.3450, 2.3643, 2.3769, 2.3568,\n",
            "        2.3886], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 115 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2657, 3.2583, 3.2750, 3.2322, 3.2584, 3.2708, 3.2840, 3.2359, 3.2413,\n",
            "        3.2452], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3746, 2.3869, 2.3460, 2.4113, 2.3361, 2.3923, 2.3872, 2.4152, 2.4112,\n",
            "        2.3864], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 116 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2678, 3.2293, 3.2408, 3.2651, 3.2195, 3.2110, 3.2713, 3.2444, 3.2445,\n",
            "        3.2484], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3194, 2.3742, 2.3566, 2.3343, 2.3610, 2.3937, 2.3363, 2.3563, 2.3559,\n",
            "        2.3632], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 117 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2579, 3.2230, 3.2611, 3.2198, 3.2348, 3.2326, 3.2416, 3.2506, 3.2491,\n",
            "        3.2152], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3951, 2.3985, 2.4056, 2.3972, 2.3997, 2.3973, 2.3826, 2.3903, 2.4005,\n",
            "        2.4012], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 118 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2704, 3.2302, 3.2665, 3.2734, 3.2703, 3.2715, 3.2378, 3.2127, 3.2263,\n",
            "        3.2449], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3698, 2.4029, 2.3588, 2.3679, 2.3687, 2.3587, 2.4089, 2.4364, 2.4054,\n",
            "        2.4092], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 119 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2628, 3.2317, 3.2347, 3.2785, 3.2285, 3.2517, 3.2513, 3.2595, 3.2483,\n",
            "        3.2510], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3521, 2.3443, 2.3866, 2.3311, 2.4065, 2.3227, 2.3317, 2.3320, 2.3613,\n",
            "        2.3473], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 120 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2723, 3.2596, 3.2509, 3.2430, 3.2629, 3.2351, 3.2870, 3.2594, 3.2796,\n",
            "        3.2479], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3848, 2.3884, 2.3981, 2.3887, 2.3858, 2.4164, 2.3444, 2.3464, 2.3671,\n",
            "        2.3897], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 121 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1917, 3.2328, 3.1917, 3.2094, 3.2280, 3.2434, 3.2423, 3.2396, 3.1816,\n",
            "        3.2489], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3750, 2.3704, 2.3919, 2.3718, 2.3646, 2.3533, 2.3840, 2.3640, 2.3773,\n",
            "        2.3353], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 122 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2450, 3.2763, 3.2470, 3.2342, 3.2467, 3.2801, 3.2563, 3.2547, 3.2162,\n",
            "        3.2588], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3720, 2.3581, 2.3440, 2.3588, 2.3853, 2.3714, 2.3895, 2.3734, 2.3991,\n",
            "        2.3641], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 123 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2551, 3.2736, 3.2551, 3.2356, 3.2361, 3.2593, 3.2685, 3.2407, 3.2415,\n",
            "        3.2544], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3340, 2.3027, 2.3333, 2.3444, 2.3592, 2.3148, 2.3110, 2.3457, 2.3408,\n",
            "        2.3411], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 124 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2641, 3.2586, 3.2756, 3.2777, 3.2324, 3.2400, 3.2659, 3.2772, 3.2623,\n",
            "        3.2013], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3601, 2.3727, 2.3536, 2.3701, 2.3662, 2.3463, 2.3707, 2.3524, 2.3870,\n",
            "        2.4005], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 125 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2294, 3.2487, 3.2333, 3.2755, 3.2374, 3.2576, 3.2265, 3.2625, 3.2744,\n",
            "        3.2439], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3381, 2.3229, 2.3477, 2.3131, 2.3255, 2.3298, 2.3465, 2.3085, 2.3222,\n",
            "        2.3296], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 126 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2417, 3.2555, 3.2372, 3.2616, 3.2318, 3.2459, 3.2343, 3.2197, 3.2426,\n",
            "        3.2752], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3722, 2.3564, 2.3682, 2.3649, 2.4030, 2.3619, 2.3926, 2.3953, 2.3670,\n",
            "        2.3164], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 127 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2326, 3.2775, 3.2597, 3.2187, 3.2696, 3.2348, 3.2697, 3.2254, 3.2447,\n",
            "        3.2409], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4133, 2.3514, 2.3946, 2.3949, 2.3777, 2.3861, 2.3451, 2.3999, 2.3720,\n",
            "        2.4114], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 128 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2649, 3.2422, 3.2534, 3.2358, 3.2496, 3.2532, 3.2285, 3.2430, 3.2161,\n",
            "        3.2521], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3279, 2.3414, 2.3276, 2.3445, 2.3308, 2.3361, 2.3473, 2.3362, 2.3468,\n",
            "        2.3197], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 129 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2126, 3.2406, 3.2563, 3.2323, 3.2781, 3.2258, 3.2319, 3.2105, 3.2537,\n",
            "        3.2296], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4036, 2.3841, 2.3557, 2.3692, 2.3282, 2.4067, 2.3798, 2.4216, 2.3783,\n",
            "        2.3738], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 130 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2716, 3.2676, 3.2656, 3.2653, 3.2154, 3.1936, 3.2561, 3.2300, 3.2512,\n",
            "        3.2424], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4167, 2.3794, 2.3829, 2.3568, 2.3868, 2.4001, 2.3857, 2.4038, 2.4065,\n",
            "        2.4069], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 131 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2217, 3.2526, 3.2797, 3.2296, 3.2383, 3.2191, 3.2710, 3.2336, 3.2453,\n",
            "        3.2694], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3733, 2.3454, 2.3349, 2.3639, 2.3523, 2.3857, 2.3287, 2.3625, 2.3651,\n",
            "        2.3334], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 132 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2692, 3.2466, 3.2348, 3.2555, 3.2773, 3.2276, 3.2620, 3.2322, 3.2460,\n",
            "        3.2393], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3611, 2.3629, 2.4122, 2.3596, 2.3298, 2.4165, 2.3654, 2.3743, 2.3711,\n",
            "        2.4161], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 133 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2541, 3.2284, 3.1995, 3.2247, 3.2278, 3.2552, 3.2513, 3.2548, 3.2258,\n",
            "        3.2205], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3969, 2.3880, 2.4018, 2.3910, 2.3995, 2.4041, 2.3912, 2.3927, 2.3921,\n",
            "        2.4053], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 134 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2431, 3.2588, 3.2542, 3.2286, 3.2057, 3.2141, 3.2742, 3.2336, 3.2184,\n",
            "        3.2796], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3764, 2.3513, 2.3667, 2.3491, 2.3909, 2.3726, 2.3293, 2.3614, 2.3918,\n",
            "        2.3286], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 135 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2371, 3.2042, 3.2466, 3.2619, 3.2355, 3.2575, 3.2386, 3.2237, 3.2414,\n",
            "        3.2097], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4131, 2.4032, 2.3848, 2.3721, 2.4007, 2.3763, 2.3968, 2.3929, 2.3900,\n",
            "        2.4039], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 136 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2566, 3.2221, 3.2381, 3.2418, 3.2491, 3.2296, 3.2723, 3.2732, 3.2565,\n",
            "        3.2438], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3174, 2.3600, 2.3697, 2.3600, 2.3167, 2.3678, 2.3011, 2.3240, 2.3362,\n",
            "        2.3436], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 137 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2595, 3.2566, 3.2135, 3.2671, 3.2605, 3.2669, 3.2489, 3.2364, 3.2533,\n",
            "        3.2562], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3422, 2.3528, 2.4258, 2.3213, 2.3287, 2.3180, 2.3329, 2.4073, 2.3293,\n",
            "        2.3347], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 138 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2634, 3.2186, 3.2340, 3.2603, 3.2479, 3.2611, 3.2511, 3.2484, 3.2252,\n",
            "        3.2429], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3336, 2.3862, 2.3685, 2.3725, 2.3515, 2.3360, 2.3330, 2.3616, 2.3859,\n",
            "        2.3492], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 139 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2782, 3.2365, 3.2740, 3.2543, 3.2673, 3.2745, 3.2493, 3.2448, 3.2486,\n",
            "        3.2835], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3536, 2.3707, 2.3684, 2.3894, 2.3677, 2.3924, 2.3612, 2.4090, 2.3889,\n",
            "        2.3884], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 140 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2401, 3.2343, 3.2480, 3.2699, 3.2328, 3.2466, 3.2239, 3.2484, 3.2552,\n",
            "        3.2475], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3602, 2.3731, 2.3492, 2.3376, 2.3735, 2.3551, 2.3686, 2.3578, 2.3624,\n",
            "        2.3344], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 141 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2560, 3.2281, 3.2388, 3.2757, 3.2633, 3.2750, 3.2697, 3.2520, 3.2362,\n",
            "        3.2708], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3625, 2.4301, 2.4111, 2.3573, 2.3959, 2.3559, 2.3299, 2.3906, 2.3907,\n",
            "        2.3748], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 142 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2374, 3.2591, 3.2074, 3.2033, 3.2415, 3.2620, 3.2454, 3.2557, 3.2408,\n",
            "        3.2463], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3610, 2.3188, 2.3972, 2.3702, 2.3526, 2.3246, 2.3685, 2.3282, 2.3615,\n",
            "        2.3464], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 143 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2658, 3.2721, 3.2675, 3.2418, 3.2373, 3.2497, 3.2382, 3.2615, 3.2514,\n",
            "        3.2160], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3384, 2.3108, 2.3237, 2.3445, 2.4094, 2.4006, 2.4068, 2.3936, 2.3862,\n",
            "        2.4241], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 144 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2460, 3.2317, 3.2152, 3.2334, 3.2457, 3.2429, 3.2601, 3.2209, 3.2240,\n",
            "        3.2344], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3702, 2.3702, 2.4080, 2.3856, 2.3779, 2.3587, 2.3775, 2.3955, 2.3791,\n",
            "        2.3806], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 145 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2195, 3.2128, 3.1985, 3.2541, 3.2655, 3.2699, 3.2317, 3.2398, 3.2411,\n",
            "        3.2484], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3899, 2.3754, 2.4287, 2.3453, 2.3968, 2.3291, 2.3701, 2.3803, 2.3593,\n",
            "        2.3536], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 146 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2669, 3.2530, 3.1966, 3.2645, 3.2593, 3.2253, 3.2256, 3.2755, 3.2447,\n",
            "        3.2366], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3769, 2.3699, 2.3980, 2.3620, 2.3716, 2.4015, 2.4036, 2.3621, 2.3764,\n",
            "        2.3796], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 147 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2136, 3.2104, 3.2109, 3.2467, 3.2258, 3.2289, 3.1842, 3.1894, 3.2081,\n",
            "        3.2481], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3725, 2.3759, 2.3614, 2.3331, 2.3683, 2.3337, 2.3820, 2.3782, 2.3711,\n",
            "        2.3548], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 148 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2339, 3.2172, 3.2210, 3.2428, 3.2672, 3.2505, 3.2580, 3.2350, 3.2144,\n",
            "        3.2300], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3787, 2.3808, 2.3993, 2.3611, 2.3721, 2.4008, 2.3674, 2.3969, 2.4335,\n",
            "        2.3689], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 149 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2148, 3.2514, 3.2120, 3.2534, 3.2517, 3.2436, 3.2323, 3.2585, 3.2507,\n",
            "        3.2623], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3409, 2.3469, 2.3423, 2.3265, 2.3712, 2.3087, 2.3628, 2.3228, 2.3500,\n",
            "        2.3281], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 150 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2766, 3.2509, 3.2560, 3.2579, 3.2391, 3.2433, 3.2168, 3.2558, 3.2227,\n",
            "        3.2764], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3756, 2.4043, 2.3639, 2.3796, 2.3869, 2.3916, 2.4083, 2.3650, 2.3966,\n",
            "        2.3519], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 151 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2658, 3.2407, 3.2368, 3.2369, 3.2418, 3.2622, 3.2632, 3.2511, 3.2446,\n",
            "        3.2320], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3168, 2.3180, 2.3601, 2.3550, 2.3355, 2.3166, 2.3149, 2.3213, 2.3307,\n",
            "        2.3378], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 152 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2530, 3.2281, 3.2416, 3.2094, 3.2285, 3.2422, 3.2457, 3.2609, 3.2367,\n",
            "        3.2141], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3726, 2.4058, 2.3738, 2.4173, 2.3980, 2.3791, 2.3827, 2.3497, 2.3838,\n",
            "        2.4019], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 153 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2217, 3.2616, 3.2603, 3.2634, 3.2554, 3.2403, 3.2480, 3.2471, 3.2570,\n",
            "        3.2527], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3331, 2.3178, 2.3054, 2.3248, 2.3167, 2.3163, 2.3205, 2.3239, 2.3264,\n",
            "        2.3303], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 154 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2537, 3.2152, 3.2384, 3.2666, 3.2199, 3.2580, 3.2554, 3.2247, 3.2445,\n",
            "        3.2450], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3802, 2.4083, 2.3852, 2.3785, 2.4087, 2.3886, 2.3743, 2.3882, 2.3526,\n",
            "        2.3526], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 155 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2380, 3.2396, 3.2676, 3.2205, 3.2351, 3.2702, 3.2374, 3.2257, 3.2795,\n",
            "        3.2673], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3241, 2.3541, 2.3562, 2.3529, 2.3428, 2.2900, 2.3707, 2.3380, 2.3129,\n",
            "        2.3194], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 156 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2479, 3.2506, 3.2646, 3.2651, 3.2345, 3.2492, 3.2697, 3.2404, 3.2600,\n",
            "        3.2110], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3845, 2.3653, 2.3856, 2.3626, 2.4016, 2.3890, 2.3649, 2.4130, 2.3869,\n",
            "        2.4073], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 157 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2452, 3.2351, 3.2171, 3.2615, 3.2360, 3.2444, 3.2497, 3.2415, 3.2358,\n",
            "        3.2418], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3227, 2.3603, 2.3631, 2.3174, 2.3497, 2.3449, 2.3300, 2.3160, 2.3452,\n",
            "        2.3217], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 158 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2670, 3.2541, 3.2723, 3.2377, 3.2421, 3.2259, 3.2724, 3.2361, 3.2238,\n",
            "        3.2423], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3494, 2.3650, 2.3615, 2.3859, 2.3724, 2.3715, 2.3360, 2.3796, 2.3925,\n",
            "        2.3778], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 159 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2681, 3.2293, 3.2172, 3.2457, 3.2873, 3.2698, 3.2380, 3.2408, 3.2362,\n",
            "        3.2397], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3731, 2.3941, 2.3855, 2.3911, 2.3291, 2.3636, 2.3901, 2.3813, 2.3875,\n",
            "        2.3844], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 160 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2124, 3.2036, 3.2148, 3.2484, 3.2290, 3.2206, 3.2308, 3.2064, 3.2167,\n",
            "        3.2284], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3581, 2.3621, 2.3296, 2.3286, 2.3367, 2.3510, 2.3313, 2.3323, 2.3550,\n",
            "        2.3383], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 161 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2483, 3.2052, 3.2541, 3.2224, 3.2266, 3.2438, 3.2505, 3.2525, 3.2227,\n",
            "        3.2490], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3532, 2.3853, 2.3627, 2.3901, 2.3910, 2.3444, 2.3330, 2.3568, 2.4018,\n",
            "        2.3556], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 162 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2485, 3.2612, 3.2227, 3.2482, 3.2453, 3.2518, 3.2349, 3.2030, 3.2460,\n",
            "        3.2364], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3658, 2.3570, 2.4015, 2.3867, 2.3885, 2.3747, 2.4004, 2.3928, 2.4141,\n",
            "        2.3406], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 163 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2502, 3.2268, 3.2435, 3.2448, 3.2491, 3.2074, 3.2442, 3.2634, 3.2320,\n",
            "        3.2134], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3708, 2.3732, 2.3932, 2.3931, 2.3617, 2.4193, 2.3827, 2.3727, 2.4081,\n",
            "        2.3998], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 164 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2660, 3.2575, 3.2321, 3.2537, 3.2572, 3.2107, 3.2448, 3.2709, 3.2667,\n",
            "        3.2380], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3849, 2.3821, 2.3952, 2.3700, 2.3867, 2.4156, 2.3990, 2.3580, 2.3667,\n",
            "        2.4014], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 165 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2600, 3.2151, 3.2738, 3.2469, 3.2304, 3.2350, 3.2494, 3.2308, 3.2359,\n",
            "        3.2780], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3333, 2.3884, 2.3314, 2.3806, 2.3656, 2.3787, 2.3748, 2.3602, 2.3844,\n",
            "        2.3755], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 166 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2556, 3.2556, 3.2595, 3.2259, 3.2696, 3.2652, 3.2423, 3.2313, 3.2517,\n",
            "        3.2756], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3772, 2.3759, 2.3817, 2.3927, 2.3895, 2.3872, 2.3985, 2.3999, 2.3902,\n",
            "        2.3407], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 167 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2229, 3.2386, 3.2713, 3.1832, 3.2092, 3.1795, 3.2672, 3.1830, 3.2095,\n",
            "        3.2415], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3527, 2.3590, 2.3410, 2.4106, 2.3629, 2.4164, 2.3097, 2.4002, 2.3548,\n",
            "        2.3489], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 168 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2732, 3.2146, 3.2606, 3.2713, 3.2359, 3.2396, 3.2663, 3.2238, 3.2397,\n",
            "        3.2395], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3538, 2.3843, 2.3692, 2.3420, 2.3656, 2.3765, 2.3489, 2.3965, 2.3474,\n",
            "        2.3862], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 169 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2454, 3.2501, 3.2215, 3.2111, 3.2761, 3.2225, 3.2053, 3.2182, 3.2619,\n",
            "        3.2086], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4007, 2.4127, 2.3904, 2.4078, 2.3485, 2.4179, 2.4145, 2.4193, 2.3695,\n",
            "        2.4079], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 170 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2167, 3.2586, 3.2595, 3.2185, 3.2705, 3.2806, 3.2460, 3.2599, 3.2275,\n",
            "        3.2326], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3441, 2.3210, 2.3138, 2.3416, 2.3244, 2.3073, 2.3260, 2.3140, 2.3650,\n",
            "        2.3647], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 171 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2540, 3.2802, 3.2407, 3.2305, 3.2076, 3.2409, 3.2518, 3.2460, 3.2722,\n",
            "        3.2344], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3694, 2.3631, 2.3732, 2.4004, 2.3962, 2.3963, 2.3683, 2.3739, 2.3625,\n",
            "        2.4054], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 172 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2250, 3.2626, 3.2873, 3.2242, 3.2503, 3.2633, 3.2507, 3.2212, 3.2779,\n",
            "        3.2553], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3767, 2.3674, 2.3498, 2.3885, 2.3733, 2.3628, 2.3757, 2.3784, 2.3362,\n",
            "        2.3839], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 173 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2402, 3.2310, 3.2377, 3.2446, 3.2579, 3.2166, 3.2486, 3.2353, 3.2321,\n",
            "        3.2498], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3581, 2.3875, 2.3728, 2.3761, 2.3380, 2.3727, 2.3438, 2.3592, 2.3847,\n",
            "        2.3397], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 174 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2888, 3.2508, 3.2756, 3.2565, 3.2404, 3.2189, 3.2238, 3.2341, 3.2495,\n",
            "        3.2792], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3379, 2.3683, 2.3351, 2.3728, 2.4024, 2.3847, 2.3933, 2.3962, 2.3813,\n",
            "        2.3133], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 175 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2389, 3.2344, 3.2540, 3.2502, 3.2273, 3.2466, 3.2670, 3.2686, 3.2165,\n",
            "        3.2414], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3413, 2.3310, 2.3267, 2.3232, 2.3514, 2.3381, 2.3088, 2.3004, 2.3573,\n",
            "        2.3330], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 176 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2351, 3.2842, 3.2621, 3.2615, 3.2077, 3.2177, 3.2393, 3.2364, 3.2505,\n",
            "        3.2428], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4208, 2.3443, 2.3500, 2.3410, 2.3980, 2.4007, 2.3522, 2.4066, 2.3622,\n",
            "        2.3854], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 177 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2333, 3.3035, 3.2614, 3.2613, 3.2532, 3.2360, 3.2471, 3.2476, 3.2636,\n",
            "        3.2848], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3835, 2.3447, 2.3646, 2.3796, 2.3632, 2.3831, 2.3898, 2.3970, 2.3823,\n",
            "        2.3429], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 178 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2648, 3.2729, 3.2244, 3.2701, 3.2413, 3.2317, 3.2288, 3.2604, 3.2321,\n",
            "        3.2059], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3056, 2.3615, 2.3945, 2.3427, 2.3903, 2.3803, 2.3778, 2.3624, 2.3941,\n",
            "        2.4186], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 179 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2486, 3.2493, 3.2479, 3.2543, 3.2571, 3.2519, 3.2544, 3.2382, 3.2696,\n",
            "        3.2776], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3679, 2.4050, 2.3794, 2.3598, 2.4056, 2.4057, 2.3912, 2.4126, 2.3841,\n",
            "        2.3551], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 180 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2433, 3.2041, 3.2373, 3.2464, 3.2399, 3.2408, 3.2351, 3.2384, 3.2346,\n",
            "        3.2251], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3295, 2.3508, 2.3163, 2.3185, 2.3068, 2.3277, 2.3406, 2.3333, 2.3138,\n",
            "        2.3607], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 181 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2124, 3.2417, 3.2207, 3.2288, 3.2097, 3.2500, 3.2252, 3.2242, 3.2390,\n",
            "        3.2302], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3466, 2.3551, 2.3803, 2.3651, 2.3809, 2.3248, 2.3582, 2.3504, 2.3485,\n",
            "        2.3696], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 182 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2136, 3.2257, 3.2184, 3.2395, 3.2451, 3.2054, 3.2566, 3.2522, 3.2388,\n",
            "        3.2252], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3793, 2.3755, 2.3850, 2.3679, 2.3597, 2.3986, 2.3744, 2.3878, 2.3691,\n",
            "        2.3755], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 183 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2206, 3.2366, 3.2568, 3.2642, 3.2533, 3.2583, 3.2591, 3.2429, 3.2518,\n",
            "        3.2426], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3824, 2.3711, 2.3503, 2.3721, 2.3770, 2.3480, 2.3593, 2.3775, 2.3887,\n",
            "        2.3680], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 184 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2414, 3.2099, 3.2339, 3.2641, 3.2221, 3.2417, 3.2494, 3.2609, 3.2472,\n",
            "        3.2253], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3449, 2.4105, 2.3818, 2.3750, 2.3809, 2.4298, 2.3454, 2.3537, 2.3549,\n",
            "        2.3915], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 185 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2327, 3.2582, 3.2510, 3.2411, 3.2458, 3.2489, 3.2671, 3.2588, 3.2779,\n",
            "        3.2404], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4068, 2.3483, 2.4145, 2.3786, 2.3914, 2.3816, 2.3859, 2.3718, 2.3732,\n",
            "        2.3799], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 186 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2370, 3.2199, 3.2299, 3.2633, 3.2436, 3.2297, 3.2527, 3.2184, 3.2637,\n",
            "        3.2265], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4068, 2.3923, 2.3883, 2.3669, 2.3718, 2.3980, 2.3988, 2.3942, 2.3653,\n",
            "        2.3841], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 187 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2163, 3.2534, 3.2254, 3.2159, 3.2062, 3.2195, 3.2532, 3.2201, 3.2510,\n",
            "        3.2446], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3477, 2.3039, 2.3236, 2.3578, 2.3686, 2.3627, 2.3003, 2.3711, 2.3141,\n",
            "        2.3424], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 188 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2229, 3.2587, 3.2344, 3.2337, 3.2433, 3.2614, 3.2745, 3.2701, 3.2554,\n",
            "        3.2408], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4058, 2.3617, 2.3941, 2.3971, 2.3994, 2.3540, 2.3749, 2.3572, 2.3590,\n",
            "        2.3703], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 189 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2326, 3.2256, 3.2326, 3.2432, 3.2531, 3.2358, 3.2656, 3.2769, 3.2523,\n",
            "        3.2715], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3348, 2.3745, 2.3578, 2.3227, 2.2911, 2.3529, 2.3053, 2.2924, 2.3033,\n",
            "        2.3178], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 190 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2285, 3.2178, 3.2491, 3.2060, 3.2331, 3.2657, 3.2537, 3.2542, 3.2139,\n",
            "        3.2707], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4040, 2.3898, 2.3943, 2.4209, 2.4084, 2.3816, 2.3853, 2.3779, 2.3996,\n",
            "        2.3531], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 191 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2728, 3.2430, 3.2216, 3.2171, 3.2508, 3.2829, 3.2472, 3.2549, 3.2499,\n",
            "        3.2619], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3288, 2.3516, 2.3704, 2.3527, 2.3675, 2.3262, 2.3602, 2.3749, 2.3501,\n",
            "        2.3329], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 192 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2652, 3.2387, 3.2025, 3.2263, 3.2656, 3.1929, 3.2338, 3.2065, 3.2516,\n",
            "        3.2450], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3028, 2.3246, 2.3771, 2.3509, 2.3322, 2.3922, 2.3371, 2.3513, 2.3441,\n",
            "        2.3372], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 193 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2066, 3.2588, 3.2466, 3.2416, 3.2313, 3.2441, 3.2432, 3.2233, 3.2503,\n",
            "        3.2587], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4148, 2.3792, 2.3874, 2.3732, 2.4159, 2.3965, 2.3942, 2.4065, 2.3674,\n",
            "        2.3632], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 194 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2666, 3.2576, 3.2581, 3.2181, 3.1979, 3.2243, 3.2478, 3.2420, 3.2132,\n",
            "        3.2102], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3671, 2.3612, 2.3683, 2.4082, 2.4153, 2.3848, 2.3725, 2.3801, 2.3981,\n",
            "        2.4057], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 195 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2145, 3.2627, 3.2531, 3.2204, 3.2573, 3.2616, 3.2671, 3.2391, 3.2315,\n",
            "        3.2587], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4335, 2.3827, 2.3958, 2.4215, 2.3792, 2.3894, 2.3804, 2.3816, 2.4021,\n",
            "        2.3971], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 196 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2167, 3.2336, 3.2510, 3.2144, 3.2470, 3.2378, 3.2805, 3.2486, 3.2205,\n",
            "        3.2157], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3655, 2.3344, 2.3277, 2.3874, 2.3482, 2.3305, 2.3026, 2.3345, 2.3629,\n",
            "        2.3436], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 197 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2608, 3.2393, 3.2023, 3.2540, 3.2203, 3.2461, 3.2810, 3.2739, 3.2464,\n",
            "        3.2271], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3926, 2.3716, 2.3890, 2.3797, 2.3891, 2.3656, 2.3387, 2.3324, 2.3670,\n",
            "        2.3681], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 198 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2671, 3.2431, 3.2344, 3.2520, 3.2514, 3.2309, 3.2413, 3.2317, 3.2593,\n",
            "        3.2523], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3388, 2.3641, 2.3724, 2.3216, 2.3421, 2.3513, 2.3546, 2.3714, 2.3365,\n",
            "        2.3415], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 199 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2281, 3.2299, 3.2688, 3.2280, 3.2529, 3.2567, 3.2427, 3.2605, 3.2324,\n",
            "        3.2288], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3872, 2.3992, 2.3521, 2.3730, 2.3496, 2.3719, 2.3599, 2.3681, 2.3524,\n",
            "        2.3767], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 200 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2546, 3.2505, 3.2835, 3.2337, 3.2402, 3.2544, 3.2474, 3.2258, 3.2501,\n",
            "        3.2360], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3754, 2.3892, 2.3665, 2.3873, 2.3499, 2.3937, 2.3894, 2.4058, 2.3892,\n",
            "        2.4043], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 201 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2520, 3.2345, 3.2620, 3.2448, 3.2339, 3.2440, 3.2176, 3.2350, 3.2490,\n",
            "        3.2293], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3342, 2.3513, 2.3135, 2.3311, 2.3337, 2.3487, 2.3571, 2.3401, 2.3682,\n",
            "        2.3261], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 202 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2639, 3.2523, 3.2449, 3.2639, 3.2380, 3.2502, 3.2878, 3.2633, 3.2450,\n",
            "        3.2454], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3540, 2.3444, 2.3468, 2.3535, 2.3725, 2.3445, 2.3344, 2.3432, 2.3877,\n",
            "        2.3551], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 203 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2503, 3.2583, 3.2491, 3.2428, 3.2956, 3.2552, 3.2882, 3.2327, 3.2389,\n",
            "        3.2869], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3905, 2.3790, 2.3813, 2.3774, 2.3592, 2.3929, 2.3436, 2.3909, 2.4009,\n",
            "        2.3848], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 204 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2018, 3.2052, 3.2254, 3.2067, 3.2332, 3.2732, 3.2278, 3.2692, 3.2179,\n",
            "        3.2737], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3816, 2.3731, 2.3708, 2.3728, 2.3822, 2.3188, 2.3568, 2.3037, 2.3860,\n",
            "        2.3469], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 205 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2699, 3.2802, 3.2829, 3.3080, 3.2319, 3.2641, 3.2267, 3.2936, 3.2810,\n",
            "        3.2299], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3520, 2.3805, 2.3641, 2.3425, 2.4068, 2.3606, 2.4053, 2.3547, 2.3732,\n",
            "        2.3872], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 206 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2757, 3.2476, 3.2182, 3.2421, 3.2296, 3.2653, 3.2577, 3.2630, 3.2706,\n",
            "        3.1943], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3195, 2.3559, 2.3674, 2.3806, 2.3793, 2.3173, 2.3588, 2.3345, 2.3272,\n",
            "        2.3768], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 207 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2448, 3.2093, 3.2609, 3.2798, 3.2345, 3.2624, 3.2633, 3.2094, 3.2326,\n",
            "        3.2591], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3817, 2.4043, 2.3824, 2.3709, 2.4248, 2.3790, 2.3717, 2.4195, 2.4015,\n",
            "        2.3581], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 208 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2362, 3.2448, 3.2334, 3.2353, 3.2459, 3.2514, 3.2357, 3.2478, 3.2175,\n",
            "        3.2119], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3776, 2.3192, 2.3536, 2.3511, 2.3388, 2.3394, 2.3422, 2.3538, 2.3555,\n",
            "        2.3572], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 209 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2604, 3.2250, 3.2478, 3.1950, 3.2430, 3.2472, 3.2390, 3.2604, 3.2374,\n",
            "        3.2824], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3349, 2.3970, 2.3628, 2.3857, 2.3767, 2.3366, 2.3944, 2.3537, 2.3777,\n",
            "        2.3515], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 210 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2638, 3.2292, 3.2495, 3.2327, 3.2592, 3.2557, 3.2731, 3.2224, 3.2472,\n",
            "        3.2580], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4028, 2.3850, 2.3351, 2.4081, 2.3945, 2.3918, 2.3802, 2.3956, 2.3695,\n",
            "        2.3895], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 211 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2312, 3.2128, 3.2493, 3.2816, 3.2382, 3.2485, 3.2644, 3.2357, 3.2623,\n",
            "        3.2687], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3431, 2.4076, 2.3896, 2.3451, 2.3691, 2.3956, 2.3820, 2.3862, 2.3589,\n",
            "        2.3952], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 212 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2666, 3.2469, 3.2453, 3.2483, 3.2427, 3.2539, 3.2371, 3.2542, 3.2538,\n",
            "        3.2156], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3293, 2.3965, 2.3734, 2.3485, 2.3624, 2.3540, 2.3676, 2.3818, 2.3622,\n",
            "        2.3893], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 213 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2392, 3.2446, 3.2139, 3.2544, 3.2599, 3.2391, 3.2373, 3.2238, 3.2625,\n",
            "        3.2838], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4194, 2.3923, 2.4154, 2.3847, 2.3909, 2.3721, 2.3706, 2.4120, 2.3933,\n",
            "        2.3465], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 214 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2656, 3.2732, 3.2600, 3.2563, 3.2741, 3.2402, 3.2320, 3.2582, 3.2690,\n",
            "        3.2452], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3330, 2.3309, 2.3183, 2.3214, 2.3179, 2.3571, 2.3512, 2.3246, 2.3278,\n",
            "        2.3381], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 215 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2598, 3.2635, 3.2498, 3.2569, 3.2633, 3.2486, 3.2420, 3.2666, 3.2429,\n",
            "        3.2538], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3517, 2.3375, 2.3512, 2.3468, 2.3546, 2.3636, 2.3579, 2.3796, 2.3792,\n",
            "        2.3725], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 216 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2497, 3.2588, 3.2558, 3.2824, 3.2436, 3.2030, 3.2440, 3.2410, 3.2268,\n",
            "        3.2833], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3856, 2.3716, 2.4039, 2.3260, 2.3611, 2.4054, 2.3842, 2.4063, 2.4024,\n",
            "        2.3642], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 217 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2039, 3.2398, 3.2290, 3.2023, 3.2328, 3.2422, 3.2511, 3.2715, 3.2757,\n",
            "        3.2428], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3940, 2.3396, 2.3478, 2.3684, 2.3640, 2.3059, 2.3464, 2.2961, 2.2843,\n",
            "        2.3170], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 218 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2690, 3.2451, 3.2777, 3.2552, 3.2861, 3.2929, 3.2737, 3.2385, 3.2763,\n",
            "        3.2318], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3726, 2.3989, 2.3622, 2.3745, 2.3400, 2.3225, 2.3604, 2.4124, 2.3575,\n",
            "        2.4045], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 219 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2433, 3.2478, 3.2578, 3.2718, 3.2525, 3.2164, 3.2637, 3.2609, 3.2552,\n",
            "        3.2422], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3472, 2.3425, 2.3292, 2.3209, 2.3236, 2.3675, 2.3246, 2.3149, 2.3349,\n",
            "        2.3343], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 220 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2133, 3.2767, 3.2478, 3.2405, 3.2342, 3.2369, 3.2474, 3.2770, 3.2600,\n",
            "        3.2400], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3944, 2.3512, 2.3787, 2.3795, 2.3955, 2.3831, 2.3543, 2.3295, 2.3568,\n",
            "        2.3619], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 221 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2533, 3.2708, 3.2592, 3.2324, 3.2785, 3.2640, 3.2503, 3.2786, 3.2704,\n",
            "        3.2437], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3924, 2.3874, 2.3959, 2.3882, 2.3693, 2.3663, 2.3824, 2.3801, 2.3745,\n",
            "        2.3867], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 222 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2242, 3.2259, 3.2279, 3.2272, 3.2545, 3.2449, 3.2023, 3.2047, 3.2163,\n",
            "        3.2398], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3801, 2.3873, 2.3961, 2.3890, 2.3377, 2.3455, 2.3849, 2.3506, 2.3691,\n",
            "        2.3488], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 223 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2775, 3.2648, 3.2741, 3.2184, 3.2424, 3.2671, 3.2601, 3.2259, 3.2596,\n",
            "        3.2529], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3389, 2.3971, 2.4136, 2.4245, 2.4104, 2.3447, 2.3536, 2.3868, 2.3698,\n",
            "        2.3794], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 224 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2387, 3.2590, 3.2402, 3.2461, 3.2885, 3.2401, 3.2674, 3.2395, 3.2409,\n",
            "        3.2547], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3553, 2.3307, 2.3562, 2.3320, 2.3061, 2.3545, 2.2994, 2.3318, 2.3504,\n",
            "        2.3424], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 225 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2506, 3.2323, 3.2853, 3.2128, 3.2076, 3.2208, 3.2140, 3.2082, 3.2814,\n",
            "        3.2309], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3405, 2.4097, 2.3567, 2.4211, 2.4171, 2.4015, 2.4159, 2.4099, 2.3534,\n",
            "        2.4073], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 226 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2283, 3.1767, 3.2361, 3.2349, 3.2467, 3.2161, 3.2167, 3.2142, 3.2487,\n",
            "        3.2459], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3435, 2.3821, 2.3315, 2.3707, 2.3556, 2.3761, 2.3409, 2.3317, 2.3223,\n",
            "        2.3611], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 227 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2346, 3.2029, 3.2145, 3.2225, 3.2593, 3.2641, 3.2570, 3.2536, 3.2316,\n",
            "        3.2466], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3759, 2.4079, 2.4024, 2.3546, 2.3461, 2.3456, 2.3858, 2.3832, 2.3671,\n",
            "        2.3644], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 228 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2780, 3.2607, 3.2598, 3.2535, 3.2502, 3.2598, 3.2623, 3.2605, 3.2430,\n",
            "        3.2681], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3323, 2.3695, 2.3679, 2.3560, 2.3877, 2.3634, 2.3497, 2.3745, 2.3670,\n",
            "        2.3508], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 229 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2400, 3.2371, 3.2170, 3.2610, 3.2870, 3.2784, 3.2743, 3.2561, 3.2166,\n",
            "        3.2431], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3874, 2.3875, 2.4047, 2.3779, 2.3752, 2.3571, 2.3699, 2.3815, 2.3907,\n",
            "        2.4136], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 230 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2404, 3.2185, 3.2290, 3.2573, 3.2101, 3.2169, 3.2654, 3.2351, 3.2360,\n",
            "        3.2246], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3392, 2.3378, 2.3508, 2.3050, 2.3556, 2.3663, 2.3062, 2.3513, 2.3095,\n",
            "        2.3283], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 231 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2669, 3.2633, 3.2445, 3.2281, 3.2505, 3.2568, 3.2544, 3.2584, 3.2797,\n",
            "        3.2465], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3791, 2.3420, 2.4020, 2.3859, 2.3498, 2.3784, 2.3792, 2.4170, 2.3418,\n",
            "        2.3900], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 232 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2223, 3.2542, 3.2394, 3.2760, 3.2632, 3.2363, 3.2312, 3.2517, 3.2446,\n",
            "        3.2612], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4007, 2.4067, 2.3922, 2.3391, 2.3587, 2.3963, 2.4005, 2.4081, 2.4042,\n",
            "        2.3717], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 233 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2682, 3.2412, 3.2594, 3.2645, 3.2861, 3.2497, 3.2623, 3.2423, 3.2297,\n",
            "        3.2367], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3369, 2.4069, 2.3530, 2.3406, 2.3739, 2.3781, 2.3338, 2.3642, 2.3829,\n",
            "        2.3698], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 234 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2640, 3.2584, 3.2444, 3.2439, 3.2266, 3.2317, 3.2297, 3.2861, 3.2422,\n",
            "        3.2387], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3473, 2.3748, 2.3708, 2.3961, 2.4022, 2.4077, 2.4168, 2.3807, 2.4038,\n",
            "        2.3990], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 235 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2067, 3.2057, 3.2378, 3.2321, 3.2342, 3.2210, 3.2163, 3.2529, 3.2600,\n",
            "        3.2392], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4027, 2.3941, 2.3446, 2.3517, 2.3622, 2.3927, 2.3802, 2.3654, 2.3377,\n",
            "        2.3695], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 236 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2339, 3.2472, 3.2212, 3.2416, 3.2667, 3.2222, 3.2464, 3.2476, 3.2528,\n",
            "        3.2164], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4143, 2.3777, 2.3769, 2.4216, 2.3704, 2.3874, 2.4087, 2.3735, 2.3643,\n",
            "        2.4171], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 237 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2071, 3.2427, 3.2708, 3.2735, 3.2222, 3.2399, 3.2028, 3.2095, 3.2553,\n",
            "        3.2546], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3678, 2.3519, 2.3086, 2.3240, 2.3680, 2.3596, 2.3781, 2.3573, 2.3189,\n",
            "        2.3191], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 238 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2551, 3.2484, 3.2832, 3.2811, 3.2468, 3.2204, 3.2467, 3.2616, 3.2858,\n",
            "        3.2344], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3447, 2.3700, 2.3522, 2.3422, 2.3519, 2.4026, 2.3645, 2.3659, 2.3031,\n",
            "        2.3700], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 239 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2169, 3.2208, 3.2340, 3.2316, 3.2002, 3.2118, 3.2465, 3.2536, 3.2232,\n",
            "        3.2361], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3605, 2.3551, 2.3446, 2.3502, 2.3855, 2.3669, 2.3381, 2.3438, 2.3618,\n",
            "        2.3706], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 240 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2464, 3.2590, 3.2638, 3.2522, 3.2538, 3.2862, 3.2314, 3.2366, 3.2699,\n",
            "        3.2778], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3977, 2.3755, 2.3907, 2.3552, 2.3755, 2.3355, 2.3926, 2.4014, 2.3668,\n",
            "        2.3557], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 241 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2143, 3.2183, 3.2189, 3.2828, 3.2418, 3.2073, 3.2283, 3.2296, 3.2440,\n",
            "        3.2675], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3750, 2.3512, 2.3615, 2.3229, 2.3202, 2.3872, 2.3610, 2.3592, 2.3486,\n",
            "        2.3371], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 242 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2919, 3.2820, 3.2565, 3.2721, 3.2363, 3.2494, 3.2501, 3.1906, 3.2379,\n",
            "        3.2630], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3235, 2.3397, 2.3912, 2.3819, 2.3770, 2.3450, 2.3773, 2.4040, 2.3853,\n",
            "        2.3612], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 243 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2453, 3.2117, 3.2244, 3.2172, 3.2152, 3.2633, 3.2073, 3.2484, 3.2100,\n",
            "        3.2469], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3316, 2.3590, 2.3772, 2.3828, 2.3751, 2.3188, 2.3819, 2.3242, 2.3856,\n",
            "        2.3715], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 244 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2175, 3.2382, 3.2849, 3.2149, 3.2571, 3.2685, 3.2638, 3.2401, 3.2373,\n",
            "        3.2663], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4068, 2.3913, 2.3736, 2.3995, 2.3730, 2.3826, 2.3421, 2.3700, 2.4002,\n",
            "        2.3606], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 245 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2858, 3.2166, 3.2331, 3.2437, 3.2844, 3.2525, 3.2662, 3.2271, 3.2596,\n",
            "        3.2903], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3513, 2.3954, 2.4013, 2.3848, 2.3548, 2.3955, 2.4016, 2.3882, 2.3666,\n",
            "        2.3750], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 246 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2084, 3.2616, 3.2469, 3.2497, 3.2234, 3.2478, 3.2253, 3.2513, 3.2221,\n",
            "        3.2483], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3881, 2.3463, 2.3496, 2.3690, 2.3835, 2.3545, 2.3903, 2.3393, 2.3698,\n",
            "        2.3391], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 247 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2351, 3.2539, 3.2387, 3.2792, 3.2550, 3.2476, 3.2693, 3.2615, 3.2319,\n",
            "        3.2527], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3918, 2.3633, 2.3841, 2.3690, 2.3637, 2.3733, 2.3614, 2.3792, 2.4145,\n",
            "        2.3666], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 248 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2130, 3.2547, 3.2474, 3.2515, 3.2386, 3.2493, 3.2743, 3.2331, 3.2409,\n",
            "        3.2572], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3886, 2.3802, 2.3635, 2.3898, 2.3834, 2.3763, 2.3281, 2.3964, 2.3983,\n",
            "        2.4016], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 249 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2450, 3.2293, 3.2170, 3.2611, 3.2742, 3.2537, 3.2530, 3.2792, 3.2649,\n",
            "        3.2185], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4075, 2.3984, 2.3785, 2.3679, 2.3318, 2.3504, 2.3594, 2.3294, 2.3781,\n",
            "        2.4006], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 250 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2640, 3.2512, 3.2695, 3.2778, 3.2307, 3.2640, 3.2401, 3.2503, 3.2672,\n",
            "        3.2308], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3579, 2.4102, 2.3386, 2.3787, 2.3938, 2.3570, 2.3872, 2.3660, 2.3636,\n",
            "        2.4072], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 251 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2481, 3.2435, 3.2389, 3.2436, 3.2375, 3.2377, 3.2332, 3.2409, 3.2633,\n",
            "        3.2507], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3158, 2.3524, 2.3563, 2.3610, 2.3382, 2.3481, 2.3507, 2.3608, 2.3104,\n",
            "        2.3480], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 252 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2478, 3.2824, 3.2226, 3.2715, 3.2614, 3.2601, 3.2513, 3.2729, 3.2288,\n",
            "        3.2652], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4054, 2.3677, 2.4195, 2.3644, 2.3928, 2.3761, 2.3725, 2.3644, 2.4016,\n",
            "        2.4002], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 253 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2565, 3.2209, 3.2219, 3.2694, 3.2095, 3.2028, 3.2586, 3.2487, 3.2527,\n",
            "        3.2674], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3478, 2.3726, 2.3886, 2.3109, 2.3956, 2.3871, 2.3082, 2.3606, 2.3525,\n",
            "        2.3594], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 254 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2515, 3.2421, 3.2342, 3.2277, 3.2405, 3.2267, 3.2525, 3.2441, 3.2662,\n",
            "        3.2771], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4066, 2.4110, 2.3833, 2.3886, 2.3846, 2.3864, 2.3869, 2.4007, 2.3808,\n",
            "        2.3511], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 255 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2037, 3.2457, 3.2186, 3.2764, 3.2179, 3.2434, 3.2212, 3.2459, 3.2245,\n",
            "        3.2104], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3787, 2.3348, 2.3593, 2.3138, 2.3550, 2.3438, 2.3808, 2.3122, 2.3747,\n",
            "        2.3430], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 256 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2109, 3.2598, 3.2323, 3.2039, 3.2095, 3.2109, 3.2574, 3.2647, 3.2620,\n",
            "        3.2461], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3745, 2.3472, 2.3857, 2.3864, 2.3996, 2.4064, 2.3531, 2.3719, 2.3970,\n",
            "        2.3723], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 257 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2566, 3.2309, 3.2469, 3.2453, 3.2585, 3.2578, 3.2408, 3.2417, 3.2745,\n",
            "        3.2551], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3197, 2.3512, 2.3588, 2.3327, 2.3218, 2.3523, 2.3366, 2.3174, 2.3227,\n",
            "        2.3272], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 258 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1995, 3.2245, 3.2406, 3.2059, 3.2688, 3.2594, 3.2374, 3.2371, 3.2296,\n",
            "        3.2706], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3594, 2.3821, 2.3854, 2.3891, 2.3380, 2.3215, 2.3810, 2.3800, 2.3770,\n",
            "        2.3416], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 259 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2418, 3.2732, 3.2408, 3.2405, 3.2603, 3.2423, 3.2586, 3.2488, 3.2812,\n",
            "        3.2137], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3916, 2.3715, 2.3695, 2.3885, 2.3570, 2.4045, 2.3555, 2.3714, 2.3354,\n",
            "        2.4347], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 260 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.1956, 3.2723, 3.2191, 3.2365, 3.2374, 3.2409, 3.2288, 3.2626, 3.2739,\n",
            "        3.2782], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4283, 2.3688, 2.4161, 2.4110, 2.4112, 2.3957, 2.3987, 2.3754, 2.3860,\n",
            "        2.3663], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 261 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2346, 3.2353, 3.2508, 3.2458, 3.2239, 3.2396, 3.2473, 3.2281, 3.2484,\n",
            "        3.2434], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3609, 2.3331, 2.3494, 2.3274, 2.3439, 2.3292, 2.3217, 2.3690, 2.3527,\n",
            "        2.3636], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 262 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2025, 3.2662, 3.2631, 3.2582, 3.2185, 3.2683, 3.2561, 3.2533, 3.2408,\n",
            "        3.2706], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4013, 2.3452, 2.3503, 2.3556, 2.3686, 2.3594, 2.3711, 2.3419, 2.3805,\n",
            "        2.3522], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 263 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2630, 3.2476, 3.2554, 3.2386, 3.2848, 3.2389, 3.2878, 3.2372, 3.2863,\n",
            "        3.2369], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3748, 2.3896, 2.4012, 2.3925, 2.3367, 2.3580, 2.3578, 2.3897, 2.3397,\n",
            "        2.4047], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 264 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2063, 3.2678, 3.2503, 3.2184, 3.2359, 3.2535, 3.2550, 3.2419, 3.2589,\n",
            "        3.2661], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4018, 2.3766, 2.3877, 2.4032, 2.4063, 2.3991, 2.3929, 2.4113, 2.3588,\n",
            "        2.3661], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 265 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2669, 3.2125, 3.2015, 3.2310, 3.2345, 3.2457, 3.2560, 3.2323, 3.2426,\n",
            "        3.2089], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3376, 2.4009, 2.3967, 2.3937, 2.4019, 2.4012, 2.3693, 2.3958, 2.3692,\n",
            "        2.3888], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 266 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2593, 3.2893, 3.2081, 3.2634, 3.2505, 3.2162, 3.2787, 3.2469, 3.2133,\n",
            "        3.2483], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3602, 2.3451, 2.4106, 2.3594, 2.3695, 2.4137, 2.3762, 2.3985, 2.4103,\n",
            "        2.3870], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 267 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2507, 3.2509, 3.2327, 3.2532, 3.2425, 3.2444, 3.2781, 3.2409, 3.2465,\n",
            "        3.2323], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3535, 2.3788, 2.3900, 2.3494, 2.3471, 2.3692, 2.3161, 2.3808, 2.3709,\n",
            "        2.3662], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 268 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2407, 3.2276, 3.2457, 3.2541, 3.2550, 3.2835, 3.2553, 3.2464, 3.2415,\n",
            "        3.2374], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4220, 2.3671, 2.3811, 2.3776, 2.3880, 2.3317, 2.3872, 2.3885, 2.3898,\n",
            "        2.3861], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 269 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2550, 3.2441, 3.2846, 3.2387, 3.2536, 3.2671, 3.2745, 3.2576, 3.2519,\n",
            "        3.2731], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3893, 2.3441, 2.3448, 2.3579, 2.3996, 2.3618, 2.3748, 2.3483, 2.3876,\n",
            "        2.3785], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 270 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2397, 3.2222, 3.2226, 3.2348, 3.1972, 3.2339, 3.2335, 3.2433, 3.2508,\n",
            "        3.2197], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3570, 2.3490, 2.3519, 2.3259, 2.3875, 2.3647, 2.3233, 2.3333, 2.3378,\n",
            "        2.3564], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 271 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2447, 3.2225, 3.2248, 3.2194, 3.2704, 3.2542, 3.2490, 3.2323, 3.2344,\n",
            "        3.2138], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3937, 2.3823, 2.3986, 2.3857, 2.3781, 2.3794, 2.3618, 2.4035, 2.3702,\n",
            "        2.4010], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 272 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2534, 3.2416, 3.2639, 3.2171, 3.2623, 3.2646, 3.2730, 3.2706, 3.2490,\n",
            "        3.2595], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3574, 2.4079, 2.3743, 2.3945, 2.3432, 2.3526, 2.3414, 2.3526, 2.3831,\n",
            "        2.3645], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 273 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2664, 3.2390, 3.2394, 3.2354, 3.2644, 3.2448, 3.2657, 3.2666, 3.2544,\n",
            "        3.2178], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3612, 2.3945, 2.4045, 2.3972, 2.3775, 2.3952, 2.3666, 2.3731, 2.3617,\n",
            "        2.3902], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 274 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2333, 3.2715, 3.2612, 3.2681, 3.2212, 3.2785, 3.2596, 3.2362, 3.2480,\n",
            "        3.2235], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3858, 2.3726, 2.3946, 2.3405, 2.3836, 2.3582, 2.3540, 2.3862, 2.3758,\n",
            "        2.3921], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 275 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2698, 3.2732, 3.2664, 3.2165, 3.2386, 3.2215, 3.2726, 3.2547, 3.2351,\n",
            "        3.2250], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3793, 2.3706, 2.3704, 2.4182, 2.4088, 2.4213, 2.3750, 2.3908, 2.4171,\n",
            "        2.4043], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 276 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2249, 3.2275, 3.2443, 3.3002, 3.2557, 3.2667, 3.2084, 3.2048, 3.2258,\n",
            "        3.2150], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3605, 2.3456, 2.3172, 2.2848, 2.3170, 2.3164, 2.3542, 2.3590, 2.3524,\n",
            "        2.3405], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 277 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2824, 3.2434, 3.2200, 3.2797, 3.2781, 3.2243, 3.2657, 3.2533, 3.2289,\n",
            "        3.2478], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3637, 2.3884, 2.4314, 2.3684, 2.3685, 2.4097, 2.3916, 2.3954, 2.3843,\n",
            "        2.3805], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 278 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2721, 3.2482, 3.2491, 3.2136, 3.2696, 3.2821, 3.2186, 3.2463, 3.2713,\n",
            "        3.2400], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3094, 2.3355, 2.3342, 2.3824, 2.3077, 2.2944, 2.3537, 2.3157, 2.3006,\n",
            "        2.3492], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 279 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2229, 3.2540, 3.2628, 3.1940, 3.2309, 3.2329, 3.2426, 3.2390, 3.2476,\n",
            "        3.2601], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3696, 2.3485, 2.3377, 2.3974, 2.3839, 2.3719, 2.3678, 2.3627, 2.3755,\n",
            "        2.3313], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 280 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2245, 3.2042, 3.2620, 3.2420, 3.2219, 3.2789, 3.2377, 3.2389, 3.2387,\n",
            "        3.2403], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3812, 2.4191, 2.3892, 2.3943, 2.3836, 2.3648, 2.3940, 2.3898, 2.3791,\n",
            "        2.3793], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 281 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2352, 3.2196, 3.2157, 3.2175, 3.2581, 3.2688, 3.2544, 3.2055, 3.2381,\n",
            "        3.2202], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.4142, 2.4139, 2.4053, 2.4132, 2.3669, 2.3623, 2.3834, 2.4032, 2.4008,\n",
            "        2.4155], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 282 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2335, 3.2680, 3.2362, 3.2484, 3.2419, 3.2581, 3.2467, 3.2262, 3.2399,\n",
            "        3.2380], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3486, 2.3386, 2.3321, 2.3263, 2.3467, 2.3245, 2.3504, 2.3737, 2.3475,\n",
            "        2.3369], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 283 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2622, 3.2492, 3.2158, 3.2155, 3.2338, 3.2641, 3.2565, 3.2558, 3.2334,\n",
            "        3.2773], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3393, 2.3662, 2.4097, 2.4166, 2.3902, 2.3690, 2.3758, 2.3941, 2.3994,\n",
            "        2.3438], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 284 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2412, 3.2340, 3.2446, 3.2195, 3.2464, 3.2431, 3.2497, 3.2261, 3.2622,\n",
            "        3.2357], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3459, 2.3317, 2.3140, 2.3590, 2.3300, 2.3397, 2.3288, 2.3575, 2.3240,\n",
            "        2.3511], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 285 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2266, 3.2376, 3.2177, 3.2510, 3.2398, 3.2683, 3.2455, 3.2632, 3.2300,\n",
            "        3.2676], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3569, 2.3525, 2.3871, 2.3517, 2.3678, 2.3810, 2.3812, 2.3750, 2.4055,\n",
            "        2.3604], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 286 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2591, 3.2438, 3.2312, 3.2168, 3.2551, 3.2347, 3.2465, 3.2518, 3.2340,\n",
            "        3.2600], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3233, 2.3553, 2.3536, 2.3638, 2.3143, 2.3407, 2.3361, 2.3086, 2.3265,\n",
            "        2.3179], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 287 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2180, 3.2695, 3.2101, 3.2654, 3.2484, 3.2574, 3.2609, 3.2632, 3.2341,\n",
            "        3.2423], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3806, 2.3373, 2.3834, 2.3788, 2.3628, 2.3751, 2.3874, 2.3546, 2.3823,\n",
            "        2.3811], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 288 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2613, 3.2824, 3.2588, 3.2189, 3.2181, 3.2348, 3.2555, 3.2255, 3.2003,\n",
            "        3.2553], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3679, 2.3251, 2.3800, 2.3968, 2.3991, 2.3789, 2.3781, 2.3858, 2.3799,\n",
            "        2.3751], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 289 precision : 0.0 recall : 0.0 f1 : 0.0\n",
            "tensor([3.2275, 3.2374, 3.2367, 3.2364, 3.2066, 3.2213, 3.2199, 3.2804, 3.2516,\n",
            "        3.2433], device='cuda:0', grad_fn=<SliceBackward>) tensor([2.3625, 2.3956, 2.3758, 2.3679, 2.4121, 2.3816, 2.3774, 2.3815, 2.3509,\n",
            "        2.3862], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "iter: 290 precision : 0.0 recall : 0.0 f1 : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b5bf10acd55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bitch_street_VAEflow.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model = train_flow_model(model, 1000, .001, train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_flow_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8c95c0253420>\u001b[0m in \u001b[0;36mtest_flow_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mones_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mones_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/18jtrvAqX_oAsK-p-3xkHc_htsco8ATRI/jane_street/maf.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_log_abs_det_jacobians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum_log_abs_det_jacobians\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/18jtrvAqX_oAsK-p-3xkHc_htsco8ATRI/jane_street/maf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/18jtrvAqX_oAsK-p-3xkHc_htsco8ATRI/jane_street/maf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msum_log_abs_det_jacobians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_abs_det_jacobian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0msum_log_abs_det_jacobians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_log_abs_det_jacobians\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_abs_det_jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_log_abs_det_jacobians\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/18jtrvAqX_oAsK-p-3xkHc_htsco8ATRI/jane_street/maf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# MAF eq 4 -- return mean and log std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloga\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mloga\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# MAF eq 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/18jtrvAqX_oAsK-p-3xkHc_htsco8ATRI/jane_street/maf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (1655) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02789,
          "end_time": "2021-01-10T12:36:15.587362",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.559472",
          "status": "completed"
        },
        "tags": [],
        "id": "Lwwd3XyikC8j"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.801752Z",
          "iopub.status.busy": "2021-01-10T12:36:15.801156Z",
          "iopub.status.idle": "2021-01-10T12:36:15.827964Z",
          "shell.execute_reply": "2021-01-10T12:36:15.827319Z"
        },
        "papermill": {
          "duration": 0.048712,
          "end_time": "2021-01-10T12:36:15.828095",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.779383",
          "status": "completed"
        },
        "tags": [],
        "id": "IlJhLwZBkC8k"
      },
      "source": [
        "import janestreet\n",
        "env = janestreet.make_env()\n",
        "env_iter = env.iter_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.879534Z",
          "iopub.status.busy": "2021-01-10T12:36:15.876909Z",
          "iopub.status.idle": "2021-01-10T12:41:08.393388Z",
          "shell.execute_reply": "2021-01-10T12:41:08.393959Z"
        },
        "papermill": {
          "duration": 292.546472,
          "end_time": "2021-01-10T12:41:08.394171",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.847699",
          "status": "completed"
        },
        "tags": [],
        "id": "ckZO4q9qkC8k"
      },
      "source": [
        "opt_th = 0.5\n",
        "\n",
        "if not train_mode:\n",
        "    for (test_df, pred_df) in env_iter:\n",
        "        \n",
        "        if test_df['weight'].item() > 0:\n",
        "            test_df = test_df.loc[:, features].values\n",
        "            if np.isnan(test_df[:, 1:].sum()):\n",
        "                test_df[:, 1:] = np.nan_to_num(test_df[:, 1:]) + np.isnan(test_df[:, 1:]) * f_mean\n",
        "\n",
        "            pred_vector = np.mean([fit(model, test_df) for model in models],axis=0)\n",
        "            pred = np.mean(pred_vector)\n",
        "            pred_df.action = (pred_vector > opt_th).astype(int) \n",
        "            \n",
        "\n",
        "        else:\n",
        "            pred_df.action = 0\n",
        "        env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.019307,
          "end_time": "2021-01-10T12:41:08.433875",
          "exception": false,
          "start_time": "2021-01-10T12:41:08.414568",
          "status": "completed"
        },
        "tags": [],
        "id": "D5WWsEUhkC8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}