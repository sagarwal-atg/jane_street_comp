{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 454.162123,
      "end_time": "2021-01-10T12:41:08.662503",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-10T12:33:34.500380",
      "version": "2.1.0"
    },
    "colab": {
      "name": "pytorch-bottleneck-mlp-solution.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.653430Z",
          "iopub.status.busy": "2021-01-10T12:33:38.652736Z",
          "iopub.status.idle": "2021-01-10T12:33:38.703037Z",
          "shell.execute_reply": "2021-01-10T12:33:38.701737Z"
        },
        "papermill": {
          "duration": 0.074553,
          "end_time": "2021-01-10T12:33:38.703178",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.628625",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GHDZ0YykC8A",
        "outputId": "44b121d9-3882-4410-803f-32c9b1513ffb"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/jane_street\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2021-01-10T12:33:38.743996Z",
          "iopub.status.busy": "2021-01-10T12:33:38.743340Z",
          "iopub.status.idle": "2021-01-10T12:33:41.012492Z",
          "shell.execute_reply": "2021-01-10T12:33:41.011801Z"
        },
        "papermill": {
          "duration": 2.291617,
          "end_time": "2021-01-10T12:33:41.012598",
          "exception": false,
          "start_time": "2021-01-10T12:33:38.720981",
          "status": "completed"
        },
        "tags": [],
        "id": "xDKsHz-hkC8Q"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from maf import MAF, RealNVP\n",
        "from sklearn.metrics import  precision_score, recall_score, f1_score\n",
        "from IPython import embed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import statistics \n",
        "\n",
        "LATENT_DIM = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = MinMaxScaler()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016623,
          "end_time": "2021-01-10T12:33:41.970216",
          "exception": false,
          "start_time": "2021-01-10T12:33:41.953593",
          "status": "completed"
        },
        "tags": [],
        "id": "BTwgHWZzkC8U"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZewWJ55x38OM"
      },
      "source": [
        "class CustomDataset:\n",
        "    def __init__(self, dataset, target):\n",
        "        self.dataset = dataset\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return {\n",
        "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
        "            'y': torch.tensor(self.target[item, :], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def load_data(PATH):\n",
        "    dt = pd.read_csv(PATH)\n",
        "    dt = pd.DataFrame(dt)\n",
        "    dt['action'] = (dt['resp'] > 0).astype('int')\n",
        "    dt.drop(columns=['resp', 'date', 'ts_id'], inplace=True)\n",
        "    \n",
        "    return dt\n",
        "\n",
        "data = load_data('train.csv')\n",
        "data.fillna(-1, inplace=True)\n",
        "target_column = 'action'\n",
        "feature_columns = data.columns[~data.columns.isin([target_column])]\n",
        "\n",
        "random_seed = 1\n",
        "learning_rate = 0.1\n",
        "num_epochs = 1\n",
        "batch_size = 2048\n",
        "num_features = len(feature_columns)\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "# data = scaler.fit_transform(data)\n",
        "\n",
        "\n",
        "train, validation = data[:int(len(data) * 0.75)], data[int(len(data) * 0.75):]\n",
        "train_data, train_target = train[feature_columns], train[[target_column]]\n",
        "validation_data, validation_target = validation[feature_columns], validation[[target_column]]\n",
        "train_dataset = CustomDataset(dataset=train_data.values, target=train_target.values)\n",
        "validation_dataset = CustomDataset(dataset=validation_data.values, target=validation_target.values)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.032799,
          "end_time": "2021-01-10T12:35:56.235752",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.202953",
          "status": "completed"
        },
        "tags": [],
        "id": "SL3EklSOkC8W"
      },
      "source": [
        "## 3. AutoEncoder\n",
        "THX for sharing [this great work](https://www.kaggle.com/snippsy/bottleneck-encoder-mlp-keras-tuner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.302164Z",
          "iopub.status.busy": "2021-01-10T12:35:56.296963Z",
          "iopub.status.idle": "2021-01-10T12:35:56.311109Z",
          "shell.execute_reply": "2021-01-10T12:35:56.311640Z"
        },
        "papermill": {
          "duration": 0.049526,
          "end_time": "2021-01-10T12:35:56.311774",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.262248",
          "status": "completed"
        },
        "tags": [],
        "id": "Ac5UfDgrkC8X"
      },
      "source": [
        "def softclip(tensor, min):\n",
        "    \"\"\" Clips the tensor values at the minimum value min in a softway. Taken from Handful of Trials \"\"\"\n",
        "    result_tensor = min + F.softplus(tensor - min)\n",
        "    return result_tensor\n",
        "\n",
        "class CNN_sigmaVAE(nn.Module):\n",
        "\n",
        "    def __init__(self,latent_dim=LATENT_DIM, window_size=20, use_probabilistic_decoder=False):\n",
        "        super(CNN_sigmaVAE, self).__init__()\n",
        "        \n",
        "        self.window_size=window_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.prob_decoder = use_probabilistic_decoder\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm1d(8)\n",
        "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm1d(4)\n",
        "        \n",
        "        \n",
        "        self.fc41 = nn.Linear(4*123, self.latent_dim)\n",
        "        self.fc42 = nn.Linear(4*123, self.latent_dim)\n",
        "\n",
        "        self.defc1 = nn.Linear(self.latent_dim, 4*123)\n",
        "        \n",
        "        self.deconv1 = nn.ConvTranspose1d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn1 = nn.BatchNorm1d(16)\n",
        "        self.deconv2 = nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "        self.debn2 = nn.BatchNorm1d(8)\n",
        "        self.deconv3 = nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
        "\n",
        "        self.log_sigma = 0\n",
        "        self.log_sigma = torch.nn.Parameter(torch.full((1,), 0.0)[0], requires_grad=True)\n",
        "        \n",
        "        \n",
        "        self.decoder_fc41 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc42 = nn.Linear(self.window_size, self.window_size)\n",
        "        \n",
        "        self.decoder_fc43 = nn.Linear(self.window_size, self.window_size)\n",
        "        self.decoder_fc44 = nn.Linear(self.window_size, self.window_size)\n",
        "\n",
        "        self.flow = MAF(n_blocks=1, input_size=2, cond_label_size=latent_dim, hidden_size=50, n_hidden=1)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        concat_input = x #torch.cat([x, c], 1)\n",
        "        h = self.bn1(F.relu(self.conv1(concat_input)))\n",
        "        h = self.bn2(F.relu(self.conv2(h)))\n",
        "        h = self.bn3(F.relu(self.conv3(h)))\n",
        "        \n",
        "        self.saved_dim = [h.size(1), h.size(2)]\n",
        "        \n",
        "        h = h.view(h.size(0), h.size(1) * h.size(2))\n",
        "        # from IPython import embed\n",
        "        # embed()\n",
        "        return self.fc41(h), self.fc42(h)\n",
        "    \n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add(mu) # return z sample\n",
        "    \n",
        "    def decoder(self, z):\n",
        "        concat_input = z #torch.cat([z, c], 1)\n",
        "        concat_input = self.defc1(concat_input)\n",
        "        concat_input = concat_input.view(concat_input.size(0), self.saved_dim[0], self.saved_dim[1])\n",
        "\n",
        "        h = self.debn1(F.relu(self.deconv1(concat_input)))\n",
        "        h = self.debn2(F.relu(self.deconv2(h)))     \n",
        "        out = torch.sigmoid(self.deconv3(h))\n",
        "        \n",
        "        if self.prob_decoder:\n",
        "            rec_mu = self.decoder_fc43(out).tanh()\n",
        "            rec_sigma = self.decoder_fc44(out).tanh()\n",
        "            return out, rec_mu, rec_sigma\n",
        "        \n",
        "        else:\n",
        "            return out, 0, 0\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        output, rec_mu, rec_sigma = self.decoder(z)\n",
        "\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        return output, rec_mu, rec_sigma, kl_div\n",
        "\n",
        "\n",
        "    def gaussian_nll(self, mu, log_sigma, x):\n",
        "        return 0.5 * torch.pow((x - mu) / log_sigma.exp(), 2) + log_sigma + 0.5 * np.log(2 * np.pi)\n",
        "\n",
        "    \n",
        "    def reconstruction_loss(self, x_hat, x):\n",
        "\n",
        "        log_sigma = self.log_sigma\n",
        "        log_sigma = softclip(log_sigma, -6)\n",
        "        \n",
        "        rec_comps = self.gaussian_nll(x_hat, log_sigma, x)\n",
        "        rec = rec_comps.sum()\n",
        "\n",
        "        return rec_comps, rec\n",
        "\n",
        "    \n",
        "    def loss_function(self, recon_x, x, rec_mu, rec_sigma, kl):\n",
        "        \n",
        "        rec_comps, rec = self.reconstruction_loss(recon_x, x)\n",
        "        #kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        rec_mu_sigma_loss = 0\n",
        "        if self.prob_decoder:\n",
        "            rec_mu_sigma_loss = self.gaussian_nll(rec_mu, rec_sigma, x).sum()\n",
        "        \n",
        "        return rec_comps, rec, rec_mu_sigma_loss, kl\n",
        "\n",
        "\n",
        "def train_flow_model(model, num_epochs, learning_rate, dataloader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    epochs=num_epochs\n",
        "    tq = tqdm(range(epochs))\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for iteration, batch in enumerate(dataloader):\n",
        "            inputs = batch['x'].to(device)\n",
        "            labels = batch['y'].to(device)\n",
        "            labels = torch.squeeze(labels)\n",
        "\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            mu, _ = model.encoder(inputs)\n",
        "            labels = labels.unsqueeze(1).repeat(1, 2).float()\n",
        "            zk, loss = model.flow.log_prob(x=labels, y=mu)\n",
        "\n",
        "            # VAE Training\n",
        "            # outputs, rec_mu, rec_sigma, kl = model(inputs)\n",
        "            # _, rec, _, kl = model.loss_function(outputs, inputs, rec_mu, rec_sigma, kl)\n",
        "\n",
        "            loss = -loss.mean()\n",
        "\n",
        "            if(np.isnan(loss.item())):\n",
        "                print(\"Noped out at\", epoch, j, kl, rec_comps)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(epoch, 'total :' + str(loss.item()))\n",
        "\n",
        "        torch.save(model, 'bitch_street_VAEflow.pth')\n",
        "        torch.save(model.state_dict(), 'bitch_street_VAEflow_state_dict.pth')\n",
        "\n",
        "        #break\n",
        "    return model\n",
        "\n",
        "def test_flow_model(model, dataloader):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    model.eval()\n",
        "\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    for iteration, batch in enumerate(dataloader):\n",
        "      inputs = batch['x'].to(device)\n",
        "      labels = batch['y'].to(device)\n",
        "      labels = torch.squeeze(labels)\n",
        "      # inputs = torch.from_numpy(validation_data.to_numpy()).float().to(device)\n",
        "      # labels = torch.from_numpy(validation_target.to_numpy()).float().to(device)\n",
        "\n",
        "      if inputs.shape[0] < batch_size:\n",
        "        break\n",
        "\n",
        "      labels = torch.squeeze(labels)\n",
        "      inputs = inputs.unsqueeze(1)\n",
        "\n",
        "      mu, _ = model.encoder(inputs)\n",
        "      zero_test = torch.zeros([batch_size, 2]).to(device)\n",
        "      ones_test = torch.ones_like(zero_test).to(device)\n",
        "\n",
        "      _, zero_log_prob = model.flow.log_prob(x=zero_test, y=mu)\n",
        "      _, one_log_prob = model.flow.log_prob(x=ones_test, y=mu)\n",
        "\n",
        "      z = torch.zeros([batch_size, 2]).cpu()\n",
        "      norm_zeros = scaler.fit_transform(zero_log_prob.cpu().detach().numpy().reshape(-1, 1))\n",
        "      norm_ones = scaler.fit_transform(one_log_prob.cpu().detach().numpy().reshape(-1, 1))\n",
        "\n",
        "      norm_zeros = torch.from_numpy(norm_zeros).squeeze(1)\n",
        "      norm_ones = torch.from_numpy(norm_ones).squeeze(1)\n",
        "      z[:, 0] = norm_zeros\n",
        "      z[:, 1] = norm_ones\n",
        "\n",
        "      preds = torch.argmax(z, dim=1)\n",
        "      labels = labels.cpu()\n",
        "      precision.append(precision_score(labels, preds))\n",
        "      recall.append(recall_score(labels, preds))\n",
        "      f1.append(f1_score(labels, preds))\n",
        "    print('iter: ' + str(iteration), 'precision : ' + str(statistics.mean(precision)) + ' recall : ' + str(statistics.mean(recall)) + ' f1 : ' + str(statistics.mean(f1)))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017009,
          "end_time": "2021-01-10T12:35:56.346494",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.329485",
          "status": "completed"
        },
        "tags": [],
        "id": "pRh2dlm4kC8Y"
      },
      "source": [
        "### 3-1  AutoEncoder Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:35:56.384797Z",
          "iopub.status.busy": "2021-01-10T12:35:56.383924Z",
          "iopub.status.idle": "2021-01-10T12:35:56.474255Z",
          "shell.execute_reply": "2021-01-10T12:35:56.473299Z"
        },
        "papermill": {
          "duration": 0.110676,
          "end_time": "2021-01-10T12:35:56.474418",
          "exception": false,
          "start_time": "2021-01-10T12:35:56.363742",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDhnYd8JkC8a",
        "outputId": "b07a9adb-db29-4fc6-90f2-7a52727d89f2"
      },
      "source": [
        "latent_dim=8\n",
        "model = CNN_sigmaVAE(latent_dim=latent_dim)\n",
        "model.to(device)\n",
        "model.cuda() if torch.cuda.is_available() else model.cpu()\n",
        "\n",
        "model = torch.load('bitch_street_VAEflow.pth')\n",
        "# model = train_flow_model(model, 1000, .001, train_loader)\n",
        "test_flow_model(model, validation_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 291 precision : 0.49791471534798704 recall : 0.5631578941185342 f1 : 0.5224703074519649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02789,
          "end_time": "2021-01-10T12:36:15.587362",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.559472",
          "status": "completed"
        },
        "tags": [],
        "id": "Lwwd3XyikC8j"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.801752Z",
          "iopub.status.busy": "2021-01-10T12:36:15.801156Z",
          "iopub.status.idle": "2021-01-10T12:36:15.827964Z",
          "shell.execute_reply": "2021-01-10T12:36:15.827319Z"
        },
        "papermill": {
          "duration": 0.048712,
          "end_time": "2021-01-10T12:36:15.828095",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.779383",
          "status": "completed"
        },
        "tags": [],
        "id": "IlJhLwZBkC8k"
      },
      "source": [
        "import janestreet\n",
        "env = janestreet.make_env()\n",
        "env_iter = env.iter_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-10T12:36:15.879534Z",
          "iopub.status.busy": "2021-01-10T12:36:15.876909Z",
          "iopub.status.idle": "2021-01-10T12:41:08.393388Z",
          "shell.execute_reply": "2021-01-10T12:41:08.393959Z"
        },
        "papermill": {
          "duration": 292.546472,
          "end_time": "2021-01-10T12:41:08.394171",
          "exception": false,
          "start_time": "2021-01-10T12:36:15.847699",
          "status": "completed"
        },
        "tags": [],
        "id": "ckZO4q9qkC8k"
      },
      "source": [
        "opt_th = 0.5\n",
        "\n",
        "if not train_mode:\n",
        "    for (test_df, pred_df) in env_iter:\n",
        "        \n",
        "        if test_df['weight'].item() > 0:\n",
        "            test_df = test_df.loc[:, features].values\n",
        "            if np.isnan(test_df[:, 1:].sum()):\n",
        "                test_df[:, 1:] = np.nan_to_num(test_df[:, 1:]) + np.isnan(test_df[:, 1:]) * f_mean\n",
        "\n",
        "            pred_vector = np.mean([fit(model, test_df) for model in models],axis=0)\n",
        "            pred = np.mean(pred_vector)\n",
        "            pred_df.action = (pred_vector > opt_th).astype(int) \n",
        "            \n",
        "\n",
        "        else:\n",
        "            pred_df.action = 0\n",
        "        env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.019307,
          "end_time": "2021-01-10T12:41:08.433875",
          "exception": false,
          "start_time": "2021-01-10T12:41:08.414568",
          "status": "completed"
        },
        "tags": [],
        "id": "D5WWsEUhkC8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}