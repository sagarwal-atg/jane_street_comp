{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-10T12:33:38.653430Z",
     "iopub.status.busy": "2021-01-10T12:33:38.652736Z",
     "iopub.status.idle": "2021-01-10T12:33:38.703037Z",
     "shell.execute_reply": "2021-01-10T12:33:38.701737Z"
    },
    "papermill": {
     "duration": 0.074553,
     "end_time": "2021-01-10T12:33:38.703178",
     "exception": false,
     "start_time": "2021-01-10T12:33:38.628625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/model_0.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/model_4.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/model_2.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/__results__.html\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/model_3.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/encoder.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/model_1.pkl\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/__notebook__.ipynb\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/__output__.json\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/custom.css\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/lightning_logs/version_0/events.out.tfevents.1610104740.9d3c5a7abfde.15.0\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/lightning_logs/version_0/hparams.yaml\n",
      "/kaggle/input/pytorch-jsprediction-freezed-at-v20/lightning_logs/version_0/checkpoints/epoch=9.ckpt\n",
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n",
      "/kaggle/input/js-fixed-window/model_0.pkl\n",
      "/kaggle/input/js-fixed-window/model_4.pkl\n",
      "/kaggle/input/js-fixed-window/model_2.pkl\n",
      "/kaggle/input/js-fixed-window/__results__.html\n",
      "/kaggle/input/js-fixed-window/model_3.pkl\n",
      "/kaggle/input/js-fixed-window/encoder.pkl\n",
      "/kaggle/input/js-fixed-window/model_1.pkl\n",
      "/kaggle/input/js-fixed-window/__notebook__.ipynb\n",
      "/kaggle/input/js-fixed-window/__output__.json\n",
      "/kaggle/input/js-fixed-window/custom.css\n",
      "/kaggle/input/js-fixed-window/lightning_logs/version_0/events.out.tfevents.1610272354.7b003bbdb593.15.0\n",
      "/kaggle/input/js-fixed-window/lightning_logs/version_0/hparams.yaml\n",
      "/kaggle/input/js-fixed-window/lightning_logs/version_0/checkpoints/epoch=9.ckpt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-10T12:33:38.743996Z",
     "iopub.status.busy": "2021-01-10T12:33:38.743340Z",
     "iopub.status.idle": "2021-01-10T12:33:41.012492Z",
     "shell.execute_reply": "2021-01-10T12:33:41.011801Z"
    },
    "papermill": {
     "duration": 2.291617,
     "end_time": "2021-01-10T12:33:41.012598",
     "exception": false,
     "start_time": "2021-01-10T12:33:38.720981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01687,
     "end_time": "2021-01-10T12:33:41.046699",
     "exception": false,
     "start_time": "2021-01-10T12:33:41.029829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. PurgedGroupTimeSeriesSplit\n",
    "From [here](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv), thx for sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-10T12:33:41.084900Z",
     "iopub.status.busy": "2021-01-10T12:33:41.084262Z",
     "iopub.status.idle": "2021-01-10T12:33:41.936379Z",
     "shell.execute_reply": "2021-01-10T12:33:41.935773Z"
    },
    "papermill": {
     "duration": 0.872721,
     "end_time": "2021-01-10T12:33:41.936488",
     "exception": false,
     "start_time": "2021-01-10T12:33:41.063767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016623,
     "end_time": "2021-01-10T12:33:41.970216",
     "exception": false,
     "start_time": "2021-01-10T12:33:41.953593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:33:42.024868Z",
     "iopub.status.busy": "2021-01-10T12:33:42.023996Z",
     "iopub.status.idle": "2021-01-10T12:35:56.171336Z",
     "shell.execute_reply": "2021-01-10T12:35:56.170257Z"
    },
    "papermill": {
     "duration": 134.184897,
     "end_time": "2021-01-10T12:35:56.171505",
     "exception": false,
     "start_time": "2021-01-10T12:33:41.986608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032799,
     "end_time": "2021-01-10T12:35:56.235752",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.202953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. AutoEncoder\n",
    "THX for sharing [this great work](https://www.kaggle.com/snippsy/bottleneck-encoder-mlp-keras-tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.302164Z",
     "iopub.status.busy": "2021-01-10T12:35:56.296963Z",
     "iopub.status.idle": "2021-01-10T12:35:56.311109Z",
     "shell.execute_reply": "2021-01-10T12:35:56.311640Z"
    },
    "papermill": {
     "duration": 0.049526,
     "end_time": "2021-01-10T12:35:56.311774",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.262248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AE_Dataset:\n",
    "    def __init__(self, dataset, targets):\n",
    "        self.dataset = dataset\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
    "            'y': torch.tensor(self.targets[item], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "#-------------------------------------------------------------------    \n",
    "\n",
    "class AE_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, targets, BATCH_SIZE, fold = None):\n",
    "        super().__init__()\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.fold = fold\n",
    "        \n",
    "    def preapre_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "         \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        self.train_dataset = AE_Dataset(dataset = self.data,targets = self.targets)\n",
    "        \n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, batch_size=self.BATCH_SIZE)\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return None\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return None\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Encoder\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_shape),\n",
    "            nn.Linear(input_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            #nn.Dropout(.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['x'], batch['y']\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017009,
     "end_time": "2021-01-10T12:35:56.346494",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.329485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3-1  AutoEncoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.384797Z",
     "iopub.status.busy": "2021-01-10T12:35:56.383924Z",
     "iopub.status.idle": "2021-01-10T12:35:56.474255Z",
     "shell.execute_reply": "2021-01-10T12:35:56.473299Z"
    },
    "papermill": {
     "duration": 0.110676,
     "end_time": "2021-01-10T12:35:56.474418",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.363742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mode = False\n",
    "\n",
    "EPOCHS = 10\n",
    "GPU = 0\n",
    "BATCH_SIZE = 4096\n",
    "NUM_FEATURES = X.shape[1]\n",
    "\n",
    "\n",
    "if train_mode:\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='train_loss', min_delta=0.00, patience=3, verbose=True, mode='min')\n",
    "    \n",
    "    DataLoader = AE_DataModule(data=X, targets=y, BATCH_SIZE=BATCH_SIZE)\n",
    "    trainer = pl.Trainer(gpus=GPU, max_epochs=EPOCHS, weights_summary='full', callbacks=[early_stop_callback])\n",
    "\n",
    "    AEncoder = LitAutoEncoder(input_shape=NUM_FEATURES)\n",
    "    \n",
    "    trainer.fit(AEncoder, DataLoader)\n",
    "    torch.save(AEncoder.state_dict(), 'encoder.pkl')\n",
    "\n",
    "else:\n",
    "    # https://blog.csdn.net/u012436149/article/details/68948816 \n",
    "    AEncoder = LitAutoEncoder(input_shape=NUM_FEATURES)\n",
    "    AEncoder.load_state_dict(torch.load('../input/pytorch-jsprediction-freezed-at-v20/encoder.pkl',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017144,
     "end_time": "2021-01-10T12:35:56.515070",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.497926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017244,
     "end_time": "2021-01-10T12:35:56.549718",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.532474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4-1 Define Data Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.598986Z",
     "iopub.status.busy": "2021-01-10T12:35:56.593096Z",
     "iopub.status.idle": "2021-01-10T12:35:56.603022Z",
     "shell.execute_reply": "2021-01-10T12:35:56.602204Z"
    },
    "papermill": {
     "duration": 0.035994,
     "end_time": "2021-01-10T12:35:56.603149",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.567155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Dataset:\n",
    "    def __init__(self, dataset, targets):\n",
    "        self.dataset = dataset\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            'x': torch.tensor(self.dataset[item, :], dtype=torch.float),\n",
    "            'y': torch.tensor(self.targets[item], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "#-------------------------------------------------------------------    \n",
    "\n",
    "class MLP_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, targets, BATCH_SIZE, fold = None):\n",
    "        super().__init__()\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.fold = fold\n",
    "        \n",
    "    def preapre_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        dataset = MLP_Dataset(dataset = self.data, targets = self.targets)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.BATCH_SIZE)\n",
    "        return train_loader\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        dataset = MLP_Dataset(dataset = self.data, targets = self.targets)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset,batch_size=self.BATCH_SIZE)\n",
    "        return valid_loader\n",
    "\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025897,
     "end_time": "2021-01-10T12:35:56.655596",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.629699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4-2 Define Model and Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.723066Z",
     "iopub.status.busy": "2021-01-10T12:35:56.717546Z",
     "iopub.status.idle": "2021-01-10T12:35:56.727795Z",
     "shell.execute_reply": "2021-01-10T12:35:56.726719Z"
    },
    "papermill": {
     "duration": 0.045468,
     "end_time": "2021-01-10T12:35:56.727930",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.682462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define mlp model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config, AEncoder): \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.AEncoder = AEncoder\n",
    "        self.lr = config[\"lr\"]\n",
    "        input_shape = 260\n",
    "        \n",
    "        \n",
    "        drop_out = [config[key] for key, v in config.items() if 'dropout' in key]\n",
    "        hidden_size = [config[key] for key,v in config.items() if 'layer' in key]\n",
    "        layers = [] \n",
    "        \n",
    "        for i in range(len(hidden_size)): \n",
    "            \n",
    "            out_shape = hidden_size[i]\n",
    "                # define layers\n",
    "            layers.append(nn.Dropout(drop_out[i]))\n",
    "            layers.append(nn.Linear(input_shape, out_shape))\n",
    "            layers.append(nn.BatchNorm1d(out_shape))\n",
    "            layers.append(nn.SiLU())  # SiLU aka swish\n",
    "                # update input shape\n",
    "            input_shape = out_shape\n",
    "        \n",
    "            # define the final layer\n",
    "        layers.append(nn.Dropout(drop_out[-1]))\n",
    "        layers.append(nn.Linear(input_shape, 5))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def encoder_decoder(self, x):\n",
    "        self.AEncoder.eval()\n",
    "        encoded = self.AEncoder(x)\n",
    "        decoded = self.AEncoder.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        decoded = self.encoder_decoder(x)\n",
    "        x = torch.cat((x, decoded), dim=1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.784838Z",
     "iopub.status.busy": "2021-01-10T12:35:56.772451Z",
     "iopub.status.idle": "2021-01-10T12:35:56.789443Z",
     "shell.execute_reply": "2021-01-10T12:35:56.788767Z"
    },
    "papermill": {
     "duration": 0.044061,
     "end_time": "2021-01-10T12:35:56.789549",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.745488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Train(params, num_epochs=30, batch_size=4096, patience=3):\n",
    "    loss_fn = nn.BCELoss().to(device)\n",
    "    config = {**params}\n",
    "    \n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits[2:]):\n",
    "        print('Fold : {}'.format(fold))\n",
    "        \n",
    "    # Prepare datasets\n",
    "        # train\n",
    "        tr_x, tr_y = X[train_idx], y[train_idx]\n",
    "        train_loader = MLP_DataModule(data=tr_x, targets=tr_y, BATCH_SIZE=batch_size).train_dataloader()     \n",
    "        \n",
    "        # valid\n",
    "        val_x, val_y = X[valid_idx], y[valid_idx]\n",
    "        val_loader = MLP_DataModule(data=val_x, targets=val_y, BATCH_SIZE=batch_size).valid_dataloader()\n",
    "\n",
    "        \n",
    "        # define model\n",
    "        model = MLP(config, AEncoder).to(device) #AEncoder\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr']) \n",
    "\n",
    "        # define control variables\n",
    "        the_last_loss = 100\n",
    "        trigger_times=0\n",
    "        \n",
    "    # Training \n",
    "        for epoch in range(num_epochs):   \n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "        \n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch['x'].to(device), batch['y'].to(device) \n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            # update local train loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # update global train loss\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(' Training: Epoch({}) - Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "    # Validation \n",
    "            model.eval()\n",
    "            vrunning_loss = 0.0\n",
    "            num_samples = 0\n",
    "        \n",
    "            for batch in val_loader:\n",
    "                data, labels = batch['x'].to(device), batch['y'].to(device) \n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "            \n",
    "                vrunning_loss += loss.item() * data.size(0)\n",
    "                num_samples += labels.size(0)\n",
    "\n",
    "        # update epoch loss\n",
    "            vepoch_loss = vrunning_loss/num_samples\n",
    "            print(' Validation({}) - Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "        \n",
    "        # Check if Early Stopping\n",
    "            if vepoch_loss > the_last_loss:\n",
    "                trigger_times += 1\n",
    "                if trigger_times >= patience:\n",
    "                    print('Meet Early stopping!')\n",
    "                    ##torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
    "                    break\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "                the_last_loss = vepoch_loss\n",
    "        # Save model for the best version so far\n",
    "                torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
    "        \n",
    "    \n",
    "        # Update global loss\n",
    "        Val_Loss += vepoch_loss * num_samples\n",
    "\n",
    "        # Update global # of samples \n",
    "        N_Samples += num_samples\n",
    "        \n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), f'model_{fold}.pkl')\n",
    "\n",
    "    return Val_Loss/N_Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017293,
     "end_time": "2021-01-10T12:35:56.824488",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.807195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4-3 Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:35:56.865491Z",
     "iopub.status.busy": "2021-01-10T12:35:56.864856Z",
     "iopub.status.idle": "2021-01-10T12:36:13.980765Z",
     "shell.execute_reply": "2021-01-10T12:36:13.980164Z"
    },
    "papermill": {
     "duration": 17.138877,
     "end_time": "2021-01-10T12:36:13.980870",
     "exception": false,
     "start_time": "2021-01-10T12:35:56.841993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDS=5\n",
    "gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "splits = list(gkf.split(y, groups=train['date'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:14.024636Z",
     "iopub.status.busy": "2021-01-10T12:36:14.023862Z",
     "iopub.status.idle": "2021-01-10T12:36:14.030819Z",
     "shell.execute_reply": "2021-01-10T12:36:14.030073Z"
    },
    "papermill": {
     "duration": 0.031771,
     "end_time": "2021-01-10T12:36:14.030917",
     "exception": false,
     "start_time": "2021-01-10T12:36:13.999146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:14.104747Z",
     "iopub.status.busy": "2021-01-10T12:36:14.096968Z",
     "iopub.status.idle": "2021-01-10T12:36:15.530231Z",
     "shell.execute_reply": "2021-01-10T12:36:15.530943Z"
    },
    "papermill": {
     "duration": 1.470494,
     "end_time": "2021-01-10T12:36:15.531144",
     "exception": false,
     "start_time": "2021-01-10T12:36:14.060650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining config\n",
    "config = {\n",
    "    \"layer_1_size\": 260,\n",
    "    \"layer_2_size\": 730,\n",
    "    \"layer_3_size\": 800,\n",
    "    \"layer_4_size\": 800,\n",
    "    \"layer_5_size\": 260,\n",
    "    \n",
    "    \"dropout_1\":0.1,\n",
    "    \"dropout_2\":0.7,\n",
    "    \"dropout_3\":0.8,\n",
    "    \"dropout_4\":0.45,\n",
    "    \"dropout_5\":0.60,\n",
    "    \"dropout_output\":0.5,\n",
    "    \"lr\": 1e-3\n",
    "}\n",
    "\n",
    "train_mode = False\n",
    "\n",
    "if train_mode:\n",
    "    model_loss = Train(config)\n",
    "    print(model_loss)\n",
    "    \n",
    "else:\n",
    "    models = []\n",
    "    for i in range(len(splits)):\n",
    "        mlp = MLP(config, AEncoder)\n",
    "        mlp.load_state_dict(torch.load(f'../input/pytorch-jsprediction-freezed-at-v20/model_{i}.pkl',map_location=torch.device('cpu')))\n",
    "        models.append(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02789,
     "end_time": "2021-01-10T12:36:15.587362",
     "exception": false,
     "start_time": "2021-01-10T12:36:15.559472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:15.650322Z",
     "iopub.status.busy": "2021-01-10T12:36:15.649623Z",
     "iopub.status.idle": "2021-01-10T12:36:15.653415Z",
     "shell.execute_reply": "2021-01-10T12:36:15.652873Z"
    },
    "papermill": {
     "duration": 0.038118,
     "end_time": "2021-01-10T12:36:15.653525",
     "exception": false,
     "start_time": "2021-01-10T12:36:15.615407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit function\n",
    "def fit(model, x):\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    model.eval()\n",
    "    pred = model(x)\n",
    "    return pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:15.697331Z",
     "iopub.status.busy": "2021-01-10T12:36:15.696692Z",
     "iopub.status.idle": "2021-01-10T12:36:15.759848Z",
     "shell.execute_reply": "2021-01-10T12:36:15.760382Z"
    },
    "papermill": {
     "duration": 0.088079,
     "end_time": "2021-01-10T12:36:15.760514",
     "exception": false,
     "start_time": "2021-01-10T12:36:15.672435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6915452 , 0.66107833, 0.60748076, 0.5639223 , 0.5719603 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the predict call\n",
    "mlp.eval()\n",
    "fit(mlp, X[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:15.801752Z",
     "iopub.status.busy": "2021-01-10T12:36:15.801156Z",
     "iopub.status.idle": "2021-01-10T12:36:15.827964Z",
     "shell.execute_reply": "2021-01-10T12:36:15.827319Z"
    },
    "papermill": {
     "duration": 0.048712,
     "end_time": "2021-01-10T12:36:15.828095",
     "exception": false,
     "start_time": "2021-01-10T12:36:15.779383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import janestreet\n",
    "env = janestreet.make_env()\n",
    "env_iter = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T12:36:15.879534Z",
     "iopub.status.busy": "2021-01-10T12:36:15.876909Z",
     "iopub.status.idle": "2021-01-10T12:41:08.393388Z",
     "shell.execute_reply": "2021-01-10T12:41:08.393959Z"
    },
    "papermill": {
     "duration": 292.546472,
     "end_time": "2021-01-10T12:41:08.394171",
     "exception": false,
     "start_time": "2021-01-10T12:36:15.847699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_th = 0.5\n",
    "\n",
    "if not train_mode:\n",
    "    for (test_df, pred_df) in env_iter:\n",
    "        \n",
    "        if test_df['weight'].item() > 0:\n",
    "            test_df = test_df.loc[:, features].values\n",
    "            if np.isnan(test_df[:, 1:].sum()):\n",
    "                test_df[:, 1:] = np.nan_to_num(test_df[:, 1:]) + np.isnan(test_df[:, 1:]) * f_mean\n",
    "\n",
    "            pred_vector = np.mean([fit(model, test_df) for model in models],axis=0)\n",
    "            pred = np.mean(pred_vector)\n",
    "            pred_df.action = (pred_vector > opt_th).astype(int) \n",
    "            \n",
    "\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019307,
     "end_time": "2021-01-10T12:41:08.433875",
     "exception": false,
     "start_time": "2021-01-10T12:41:08.414568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 454.162123,
   "end_time": "2021-01-10T12:41:08.662503",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-10T12:33:34.500380",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
